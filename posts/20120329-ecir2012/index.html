<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <!--
        for IE/Edge only
    -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <meta name="author" content="Manos Tsagkias">
    <link rel="canonical" href="https://manostsagkias.com/posts/20120329-ecir2012/">
    <!--
        The page_title contains the title for a page as shown in the navigation.
        Site name contains the name as defined in the mkdocs.yml
    -->
    <title>Paper at ECIR 2012 — Predicting IMDB Move Ratings Using Social Media - Connecting the Dots</title>
    <!--
        Just add a favicon.ico image to the docs.
    -->
    <link rel="shortcut icon" href="../../img/favicon.ico">
    <link rel="apple-touch-icon" sizes="180x180" href=../../img/apple-touch-icon.png>
    <link rel="icon" type="image/png" sizes="32x32" href=../../img/favicon-32x32.png>
    <link rel="icon" type="image/png" sizes="16x16" href=../../img/favicon-16x16.png>
    <link rel="manifest" href=/site.webmanifest>
      <link href="https://cdn.jsdelivr.net/npm/markdown-it-texmath/css/texmath.min.css" rel="stylesheet">
      <link href="https://cdn.jsdelivr.net/gh/goessner/mdmath/themes/publication/style.css" rel="stylesheet">
      <link href="../../css/custom.css" rel="stylesheet">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Alex+Brush&family=Allura&family=Dancing+Script:wght@400..700&family=Great+Vibes&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
    <link rel="stylesheet" href="../../css/styles.css">
    <script>
        // Inject the current file name for the active page
        const CURRENT_FILE = "posts/20120329-ecir2012/";
    </script>
</head>
<body><div id="map-overlay" class="hidden">
    
    <button id="map-close" onclick="toggleMap()">&times;</button>
    <main>
<article>
<p>
    Hover over the dots to explore related posts. Closer dots are more semantically related, and the red dot marks the current page. 
</p>
</article>
</main>
    
    <div id="map-container"></div>
    <div id="search-container">
        <input id="search-box" type="text" placeholder="Type to search the map ..." />
    </div>    
</div>

    
    <header>
        <!-- Navigation menu -->
        
        <nav>
            <ul class="nav-bar">
                <!-- Left-aligned 'Home' link -->
                <li class="nav-left">
                    <a href="../../">Home</a>
                </li>
                
                <li class="nav-right">
                    <!-- Dynamic Navigation Items from `nav` -->
                    
                        <a href="../../about/">About</a>
                    
                    <!-- Right-aligned 'Map' link -->
                    <a href="#" onclick="toggleMap()">Map</a>
                </li>
            </ul>
        </nav>
        

        
            <h1>Paper at ECIR 2012 — Predicting IMDB Move Ratings Using Social Media</h1>

<h3>Recipient of the Best Paper Award</h3>


    
        <h4>Andrei Oghina, Mathias Breuss, Manos Tsagkias, and Maarten de Rijke</h4>
    


<h5>University of Amsterdam</h5>


<h5>29 April 2012</h5>


<h5><b>Keywords:</b> conference, paper, information retrieval, predictive analytics, best paper award</h5>

        
    </header>

    <main>
        <article>
    <h3 id="abstract">Abstract<a class="headerlink" href="#abstract" title="Permanent link">&para;</a></h3>
<p>We predict IMDb movie ratings and consider two sets of features: surface and textual features. For the latter, we assume that no social media signal is isolated and use data from multiple channels that are linked to a particular movie, such as tweets from Twitter and comments from YouTube. We extract textual features from each channel to use in our prediction model and we explore whether data from either of these channels can help to extract a better set of textual feature for prediction. Our best performing model is able to rate movies very close to the observed values.</p>
<aside>
    <figure>
        <img src="/img/ecir2012-imdb-poster.png" title="Poster of the paper 'Predicting IMDB Move Ratings Using Social Media' by Andrei Oghina, Mathias Breuss, Manos Tsagkias, and Maarten de Rijke. Presented at ECIR 2012."/>
        <figcaption>Poster of the paper presented at ECIR 2012.</a></figcaption>
    <figure>
</aside>

<p>Part of the Artificial Intelligence Masters programme in which I used to teach was one month hands-on project. I have supervised Andrei Oghina and Mathias Breuss on whether <em>can we predict a movie&rsquo;s IMDB rating from social media before the movie is out</em>. Andrei and Mathias collected tweets about movies and their respective YouTube trailers, and they extracted several features which were used for training a rating classifier. We have found out that we are able predict a movie&rsquo;s IMDB rating with ±0.25 accuracy. That is if a movie gets an average IMDB rating of 8, we would predict 7.75 or 8.25. This is quite impressive for predicting a movie&rsquo;s success before it is even released.</p>
<p>Our work has been published at ECIR 2012 [<a href="#1">1</a>], and it has been awarded the best paper award; <a href="../../pdf/ecir2012-imdb.pdf">PDF (495KB)</a>.</p>
<h3 id="references">References<a class="headerlink" href="#references" title="Permanent link">&para;</a></h3>
<p><span id='1'>[1]: Andrei Oghina, Mathias Breuss, Manos Tsagkias, and Maarten de Rijke. <em>Predicting IMDB Move Ratings Using Social Media</em>. In European Conference on Information Retrieval (ECIR) 2012. <a href="https://dl.acm.org/doi/10.1007/978-3-642-28997-2_51">ACM Library</a>; <a href="../../pdf/ecir2012-imdb.pdf">PDF (495KB)</a>.</p>
</article>

<hr/>


<div class="related-links">
    <h3>Related Posts</h3>
    <ol>
        
        <li>
            
            <blockquote>
                <p>modeling. We correct for this discrepancy by introducing two
hypergeometric language models for modeling both queries, and
documents to be retrieved.</p>
<blockquote>
<p>In the second part, we focus on predicting behavior. First we look
at predicting listeners&rsquo; preference in spoken user generated
content, namely, podcasts. Then, we predict popularity of news
articles from several news agents in terms of the volume of comments
they receive. We develop models for predicting the popularity of an
article for both before and after it is published. Finally, we look
at a different aspect of news impact: how reading a news article
affects future user browsing behavior. In each setting, we find
patterns that characterize the underlying behavior and extract
features that we then use to establish models for predicting online
behavior.</p>
</blockquote>
            </blockquote>
            
            <div class="snippet-meta">
                <span class="score">25.41% similar</span>
                <span>— <a href="/posts/20121108-phd-thesis-mining-social-media/">Ph.D. Thesis — Mining Social Media: Tracking Content and Predicting Behavior</a></span>
            </div>
        </li>
        
        <li>
            
            <blockquote>
                <p>Last, an extra tip: we found that boolean scoring may be on par or outperform tf.idf or BM25 scoring in the e-commerce domain, it&rsquo;s worth checking its effectiveness on your own data ;)</p>
            </blockquote>
            
            <div class="snippet-meta">
                <span class="score">16.07% similar</span>
                <span>— <a href="/posts/20190718-ecommerce-workshop/">Inlieu of an Invited Talk—Thoughts on the Future of Search in E-commerce</a></span>
            </div>
        </li>
        
        <li>
            
            <blockquote>
                <h3 id="abstract">Abstract</h3>
<p>We focus on improving the effectiveness of a Virtual Assistant (VA) in recognizing emerging entities in spoken queries. We introduce a method that uses historical user interactions to forecast which entities will gain in popularity and become trending, and it subse- quently integrates the predictions within the Automated Speech Recognition (ASR) component of the VA. Experiments show that our proposed approach results in a 20% relative reduction in errors on emerging entity name utterances without degrading the overall recognition quality of the system.</p>
<p>Happy to share the news about my first joint pubication with the Siri Speech team at Apple. Our short paper <a href="https://arxiv.org/abs/2005.12816">Predicting Entity Popularity to Improve Spoken Entity Recognition by Virtual Assistants</a> with Christophe van Gysel, myself, Ernie Pusateri, and Ilya Oparin, is accepted at SIGIR 2020.</p>
            </blockquote>
            
            <div class="snippet-meta">
                <span class="score">15.59% similar</span>
                <span>— <a href="/posts/20200722-sigir2020/">Paper at SIGIR 2020 — Predicting Entity Popularity to Improve Spoken Entity Recognition by Virtual Assistants</a></span>
            </div>
        </li>
        
        <li>
            
            <blockquote>
                <h3 id="abstract">Abstract</h3>
<p>Modeling user browsing behavior is an active research area with
tangible real-world applications, e.g., organizations can adapt
their online presence to their visitors browsing behavior with
positive effects in user engagement, and revenue. We concentrate on
online news agents, and present a semi-supervised method for
predicting news articles that a user will visit after reading an
initial article. Our method tackles the problem using language
intent models trained on historical data which can cope
with unseen articles. We evaluate our method on a large set of
articles and in several experimental settings. Our results
demonstrate the utility of language intent models for predicting
user browsing behavior within online news sites.</p>
            </blockquote>
            
            <div class="snippet-meta">
                <span class="score">13.58% similar</span>
                <span>— <a href="/posts/20120623-sigir2012-language-intent-models/">Paper at SIGIR 2012 — Language Intent Models for Inferring User Browsing Behavior</a></span>
            </div>
        </li>
        
        <li>
            
            <blockquote>
                <p>&nbsp;
We use our pseudo test collections in two ways. First, we tune parameters of a variety of well known retrieval methods on them. Correlations with parameter sweeps on an editorial test collection are high on average, with a large variance over retrieval algorithms. Second, we use the pseudo test collections as training sets in a learning to rank scenario. Performance close to the training error on the editorial collection is achieved in all cases. Our results demonstrate the utility of tuning and training microblog search retrieval algorithms on automatically generated training material.</p>
            </blockquote>
            
            <div class="snippet-meta">
                <span class="score">12.34% similar</span>
                <span>— <a href="/posts/2013-04-08-sigir2013-pseudo-test-collections/">Paper at SIGIR 2013 — Pseudo Test Collections for Training and Tuning Microblog Rankers</a></span>
            </div>
        </li>
        
        <li>
            
            <blockquote>
                <h3 id="abstract">Abstract</h3>
<p>Language models (LMs) for virtual assistants (VAs) are typically trained on large amounts of data, resulting in prohibitively large models which require excessive memory and/or cannot be used to serve user requests in real-time. Entropy pruning results in smaller models but with significant degradation of effectiveness in the tail of the user request distribution. We customize entropy pruning by allowing for a keep list of infrequent n-grams that require a more relaxed pruning threshold, and propose three methods to construct the keep list. Each method has its own advantages and disadvantages with respect to LM size, ASR accuracy and cost of constructing the keep list. Our best LM gives 8% average Word Error Rate (WER) reduction on a targeted test set, but is 3 times larger than the baseline. We also propose discriminative methods to reduce the size of the LM while retaining the majority of the WER gains achieved by the largest LM.</p>
            </blockquote>
            
            <div class="snippet-meta">
                <span class="score">7.99% similar</span>
                <span>— <a href="/posts/20210214-icassp/">Paper at ICASSP 2021 — Error-driven Pruning of Language Models for Virtual Assistants</a></span>
            </div>
        </li>
        
        <li>
            
            <blockquote>
                <blockquote>
<p><em>6 December 2024—Note:</em> This post is a restoration of an older post, which is no longer available online. Content-wise this version is identical to the original. The original version can still be found in the <a href="http://web.archive.org/web/20130508191111/http://staff.science.uva.nl/~tsagias/?p=185">WayBack Machine</a>.</p>
</blockquote>
<div class="toc">
<ul>
<li><a href="#working-with-documents">Working with documents</a></li>
</ul>
</div>
<p>Let <span class="arithmatex"><span class="MathJax_Preview">P</span><script type="math/tex">P</script></span> and <span class="arithmatex"><span class="MathJax_Preview">Q</span><script type="math/tex">Q</script></span> be two probability distributions of a discrete random variable. If the following two properties hold:</p>
<ol>
<li>when <span class="arithmatex"><span class="MathJax_Preview">P</span><script type="math/tex">P</script></span> and <span class="arithmatex"><span class="MathJax_Preview">Q</span><script type="math/tex">Q</script></span> both sum to <span class="arithmatex"><span class="MathJax_Preview">1</span><script type="math/tex">1</script></span></li>
<li>and for any <span class="arithmatex"><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span> such that <span class="arithmatex"><span class="MathJax_Preview">P(i) &gt; 0</span><script type="math/tex">P(i) > 0</script></span> and <span class="arithmatex"><span class="MathJax_Preview">Q(i) &gt; 0</span><script type="math/tex">Q(i) > 0</script></span></li>
</ol>
<p>then, we can define their KL-divergence as:</p>
<div class="arithmatex">
<div class="MathJax_Preview">D_{KL}(P||Q) = \sum_{i}P(i)log\frac{P(i)}{Q(i)},</div>
<script type="math/tex; mode=display">D_{KL}(P||Q) = \sum_{i}P(i)log\frac{P(i)}{Q(i)},</script>
</div>
<p>and it has three properties:</p>
<ol>
<li><span class="arithmatex"><span class="MathJax_Preview">D_{KL}(P||Q) \neq D_{KL}(Q||P)</span><script type="math/tex">D_{KL}(P||Q) \neq D_{KL}(Q||P)</script></span> (asymmetry)</li>
<li>it is additive for independent distributions</li>
<li><span class="arithmatex"><span class="MathJax_Preview">D_{KL} \geq 0</span><script type="math/tex">D_{KL} \geq 0</script></span> with <span class="arithmatex"><span class="MathJax_Preview">D_{KL} = 0</span><script type="math/tex">D_{KL} = 0</script></span> iff <span class="arithmatex"><span class="MathJax_Preview">P=Q</span><script type="math/tex">P=Q</script></span></li>
</ol>
<h2 id="working-with-documents">Working with documents</h2>
            </blockquote>
            
            <div class="snippet-meta">
                <span class="score">7.77% similar</span>
                <span>— <a href="/posts/20101002-kl-divergence/">KL-divergence of Two Documents</a></span>
            </div>
        </li>
        
    </ol>
</div>

    </main>
    
    <footer>
        <p>
            Powered by MkDocs; design follows <a href="https://github.com/goessner">Stefan Gössner</a>'s <a href="https://github.com/goessner/mdmath">md-math</a>.
        </p>
    </footer>
      <script src="https://d3js.org/d3.v7.min.js"></script>
      <script src="https://cdn.jsdelivr.net/npm/dexie/dist/dexie.min.js"></script>
      <script src="../../js/d3-umap.js"></script>
      <script src="../../search/main.js"></script>
    <script type="module">
        import { search } from '/js/onnx.js';
    </script>
</body>
</html>