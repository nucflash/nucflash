<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <!--
        for IE/Edge only
    -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <meta name="author" content="Manos Tsagkias">
    <link rel="canonical" href="https://manostsagkias.com/posts/20120329-ecir2012/">
    <!--
        The page_title contains the title for a page as shown in the navigation.
        Site name contains the name as defined in the mkdocs.yml
    -->
    <title>Paper at ECIR 2012 — Predicting IMDB Move Ratings Using Social Media - Connecting the Dots</title>
    <!--
        Just add a favicon.ico image to the docs.
    -->
    <link rel="shortcut icon" href="../../img/favicon.ico">
    <link rel="apple-touch-icon" sizes="180x180" href=../../img/apple-touch-icon.png>
    <link rel="icon" type="image/png" sizes="32x32" href=../../img/favicon-32x32.png>
    <link rel="icon" type="image/png" sizes="16x16" href=../../img/favicon-16x16.png>
    <link rel="manifest" href=/site.webmanifest>
      <link href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css" rel="stylesheet">
      <link href="https://cdn.jsdelivr.net/npm/markdown-it-texmath/css/texmath.min.css" rel="stylesheet">
      <link href="https://cdn.jsdelivr.net/gh/goessner/mdmath/themes/publication/style.css" rel="stylesheet">
      <link href="../../css/custom.css" rel="stylesheet">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Alex+Brush&family=Allura&family=Dancing+Script:wght@400..700&family=Great+Vibes&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
    <link rel="stylesheet" href="../../css/styles.css">
    <script>
        // Inject the current file name for the active page
        const CURRENT_FILE = "posts/20120329-ecir2012/";
    </script>
</head>
<body><div id="map-overlay" class="hidden">
    
    <button id="map-close" onclick="toggleMap()">&times;</button>
    <main>
<article>
<p>
    Hover over the dots to explore related posts. Closer dots are more semantically related, and the red dot marks the current page. 
</p>
</article>
</main>
    
    <div id="map-container"></div>
    <div id="search-container">
        <input id="search-box" type="text" placeholder="Type to search the map ..." />
    </div>    
</div>

    
    <header>
        <!-- Navigation menu -->
        
        <nav>
            <ul class="nav-bar">
                <!-- Left-aligned 'Home' link -->
                <li class="nav-left">
                    <a href="../../">Home</a>
                </li>
                
                <li class="nav-right">
                    <!-- Dynamic Navigation Items from `nav` -->
                    
                        <a href="../../about/">About</a>
                    
                    <!-- Right-aligned 'Map' link -->
                    <a href="#" onclick="toggleMap()">Map</a>
                </li>
            </ul>
        </nav>
        

        
            <h1>Paper at ECIR 2012 — Predicting IMDB Move Ratings Using Social Media</h1>


    
        <h4>Andrei Oghina, Mathias Breuss, Manos Tsagkias, and Maarten de Rijke</h4>
    


<h5>University of Amsterdam</h5>


<h5>29 April 2012</h5>


<h5><b>Keywords:</b> conference, paper, information retrieval, predictive analytics, best paper award</h5>

        
    </header>

    <main>
        <article>
    <h3 id="abstract">Abstract<a class="headerlink" href="#abstract" title="Permanent link">&para;</a></h3>
<p>We predict IMDb movie ratings and consider two sets of features: surface and textual features. For the latter, we assume that no social media signal is isolated and use data from multiple channels that are linked to a particular movie, such as tweets from Twitter and comments from YouTube. We extract textual features from each channel to use in our prediction model and we explore whether data from either of these channels can help to extract a better set of textual feature for prediction. Our best performing model is able to rate movies very close to the observed values.</p>
<aside>
    <figure>
        <img src="/img/ecir2012-imdb-poster.png" title="Poster of the paper 'Predicting IMDB Move Ratings Using Social Media' by Andrei Oghina, Mathias Breuss, Manos Tsagkias, and Maarten de Rijke. Presented at ECIR 2012."/>
        <figcaption>Poster of the paper presented at ECIR 2012.</a></figcaption>
    <figure>
</aside>

<p>Part of the Artificial Intelligence Masters programme in which I used to teach was one month hands-on project. I have supervised Andrei Oghina and Mathias Breuss on whether <em>can we predict a movie&rsquo;s IMDB rating from social media before the movie is out</em>.</p>
<p>Andrei and Mathias collected tweets about movies and their respective YouTube trailers, and they extracted several features which were used for training a rating classifier. We have found out that we are able predict a movie&rsquo;s IMDB rating with ±0.25 accuracy. That is if a movie gets an average IMDB rating of 8, we would predict 7.75 or 8.25. This is quite impressive for predicting a movie&rsquo;s success before it is even released.</p>
<p>Our work has been published at ECIR 2012 [<a href="#1">1</a>], and it has been awarded the best paper award; <a href="../../pdf/ecir2012-imdb.pdf">PDF (495KB)</a>.</p>
<h3 id="references">References<a class="headerlink" href="#references" title="Permanent link">&para;</a></h3>
<p><span id='1'>[1]: Andrei Oghina, Mathias Breuss, Manos Tsagkias, and Maarten de Rijke. <em>Predicting IMDB Move Ratings Using Social Media</em>. In European Conference on Information Retrieval (ECIR) 2012. <a href="https://dl.acm.org/doi/10.1007/978-3-642-28997-2_51">ACM Library</a>; <a href="../../pdf/ecir2012-imdb.pdf">PDF (495KB)</a>.</p>
</article>

<hr/>


<div class="related-links">
    <h3>Related Posts</h3>
    <ol>
        
        <li>
            
            <blockquote>
                <p>retrieved, violating a standard assumption of language
modeling. We correct for this discrepancy by introducing two
hypergeometric language models for modeling both queries, and
documents to be retrieved.</p>
<blockquote>
<p>In the second part, we focus on predicting behavior. First we look
at predicting listeners&rsquo; preference in spoken user generated
content, namely, podcasts. Then, we predict popularity of news
articles from several news agents in terms of the volume of comments</p>
</blockquote>
            </blockquote>
            
            <div class="snippet-meta">
                <span class="score">23.90% similar</span>
                <span>— <a href="/posts/20121108-phd-thesis-mining-social-media/">Ph.D. Thesis — Mining Social Media: Tracking Content and Predicting Behavior</a></span>
            </div>
        </li>
        
        <li>
            
            <blockquote>
                <p>Happy to share the news about my first joint pubication with the Siri Speech team at Apple. Our short paper <a href="https://arxiv.org/abs/2005.12816">Predicting Entity Popularity to Improve Spoken Entity Recognition by Virtual Assistants</a> with Christophe van Gysel, myself, Ernie Pusateri, and Ilya Oparin, is accepted at SIGIR 2020.</p>
            </blockquote>
            
            <div class="snippet-meta">
                <span class="score">14.45% similar</span>
                <span>— <a href="/posts/20200722-sigir2020/">Paper at SIGIR 2020 — Predicting Entity Popularity to Improve Spoken Entity Recognition by Virtual Assistants</a></span>
            </div>
        </li>
        
        <li>
            
            <blockquote>
                <p>Last, an extra tip: we found that boolean scoring may be on par or outperform tf.idf or BM25 scoring in the e-commerce domain, it&rsquo;s worth checking its effectiveness on your own data ;)</p>
            </blockquote>
            
            <div class="snippet-meta">
                <span class="score">14.37% similar</span>
                <span>— <a href="/posts/20190718-ecommerce-workshop/">Inlieu of an Invited Talk—Thoughts on the Future of Search in E-commerce</a></span>
            </div>
        </li>
        
        <li>
            
            <blockquote>
                <p>intent models trained on historical data which can cope
with unseen articles. We evaluate our method on a large set of
articles and in several experimental settings. Our results
demonstrate the utility of language intent models for predicting
user browsing behavior within online news sites.</p>
            </blockquote>
            
            <div class="snippet-meta">
                <span class="score">14.16% similar</span>
                <span>— <a href="/posts/20120623-sigir2012-language-intent-models/">Paper at SIGIR 2012 — Language Intent Models for Inferring User Browsing Behavior</a></span>
            </div>
        </li>
        
        <li>
            
            <blockquote>
                <p>used hashtags, and all associated tweets as relevance judgments; we then generate a query from these tweets. Next, we generate a timestamp for each query, allowing us to use temporal information in the training process. We then enrich the generation process with knowledge derived from an editorial test collection for microblog search.</p>
            </blockquote>
            
            <div class="snippet-meta">
                <span class="score">11.60% similar</span>
                <span>— <a href="/posts/2013-04-08-sigir2013-pseudo-test-collections/">Paper at SIGIR 2013 — Pseudo Test Collections for Training and Tuning Microblog Rankers</a></span>
            </div>
        </li>
        
        <li>
            
            <blockquote>
                <p>Language models (LMs) for virtual assistants (VAs) are typically trained on large amounts of data, resulting in prohibitively large models which require excessive memory and/or cannot be used to serve user requests in real-time. Entropy pruning results in smaller models but with significant degradation of effectiveness in the tail of the user request distribution. We customize entropy pruning by allowing for a keep list of infrequent n-grams that require a more relaxed pruning threshold, and</p>
            </blockquote>
            
            <div class="snippet-meta">
                <span class="score">8.62% similar</span>
                <span>— <a href="/posts/20210214-icassp/">Paper at ICASSP 2021 — Error-driven Pruning of Language Models for Virtual Assistants</a></span>
            </div>
        </li>
        
        <li>
            
            <blockquote>
                <p>I was invited to give one of the two keynote talks. The first keynote focused on conversational agents and natural language processing and mine followed up with insights how search powers a large spectrum of applications from search, question and answering, and recommendations. I focused on the work we do at 904Labs and illustrated the principles of online learning to rank through real-world examples from our experimences with our customers.</p>
            </blockquote>
            
            <div class="snippet-meta">
                <span class="score">7.92% similar</span>
                <span>— <a href="/posts/20171021-ai2future/">Keynote speech at AI2Future</a></span>
            </div>
        </li>
        
    </ol>
</div>

    </main>
    
    <footer>
        <p>
            Powered by MkDocs; design follows <a href="https://github.com/goessner">Stefan Gössner</a>'s <a href="https://github.com/goessner/mdmath">md-math</a>.
        </p>
    </footer>
      <script src="https://unpkg.com/katex@0/dist/katex.min.js"></script>
      <script src="https://unpkg.com/katex@0/dist/contrib/auto-render.min.js"></script>
      <script src="https://d3js.org/d3.v7.min.js"></script>
      <script src="https://unpkg.com/dexie/dist/dexie.min.js"></script>
      <script src="../../js/d3-umap.js"></script>
      <script src="../../search/main.js"></script>
    <script type="module">
        import { search } from '/js/onnx.js';
    </script>
</body>
</html>