{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":""},{"location":"about/","text":"Short bio \u00b6 I design scalable systems for speech, search, recommendation, and predictive analytics, combining theory with elegant engineering to enhance user experiences. With a Ph.D. in Machine Learning and a strong foundation in Physics, I have founded three companies: MyYard, the first cloud-based ERP system for the waste management industry; 904Labs, the world\u2019s first self-learning product search engine offered as a service; and Solumbro, which introduced the first solar-powered umbrella with a virtual assistant. As an academic, my research has earned an H-index of 20, with over 55 published papers. I have co-supervised a Ph.D. thesis and guided more than 10 master\u2019s theses. Currently, I\u2019m an R&D Engineer at Apple, tackling virtual assistants at the challenging intersection of speech and search. Content Highlights Ventures Research Highlights \u00b6 904Labs self-learning search engine \u00b6 904Labs\u2019 self-learning search engine (904SLS) revolutionized e-commerce and content search as the first commercially available self-learning search engine. By optimizing search results in near real-time based on user behavior, 904SLS increased conversion rates and achieved a 30% revenue uplift for live e-commerce shops , consistently . Offering comprehensive search capabilities from query understanding to result re-ranking, 904SLS seamlessly integrates with Lucene-based infrastructures without requiring re-indexing. The team\u2019s exceptional engineering tackled challenges in machine learning, scalability, and domain-specific needs, delivering a state-of-the-art system that outperformed traditional learning-to-rank solutions. Streamwatchr \u00b6 Streamwatchr (2013\u20132016) , developed at the University of Amsterdam, used machine learning and named entity extraction to track real-time global music listening behavior from tweets. Offering features like Top-100 charts, interactive maps, and a \u201cradio\u201d stream mode powered by a dynamic recommender system, it analyzed over 438 million tweets, identifying 660,941 artists and linking them to MusicBrainz and YouTube videos. Streamwatchr\u2019s innovative engineering and real-time analytics, built with Python and MongoDB, provided unique insights into music trends and earned it a spot in the Dutch delegation at South by Southwest (SXSW). Read more about Streamwatchr . Predicting IMDB movie ratings \u00b6 In collaboration with master\u2019s students Andrei Oghina and Mathias Breuss, I supervised a hands-on project exploring whether a movie\u2019s IMDB rating could be predicted from social media activity before its release. By analyzing tweets about movies and their YouTube trailers, the team extracted features to train a rating classifier. The model achieved an impressive \u00b10.25 accuracy, predicting ratings such as 7.75 or 8.25 for movies with an average IMDB rating of 8. This groundbreaking work, published at ECIR 2012, earned the Best Paper Award for its novel approach to forecasting movie success. Read the more about the paper . Ventures \u00b6 904Labs was an Amsterdam-based artificial intelligence company that offers the first commercially available self-learning search engine. I co-founded 904Labs in 2014 and led the company until late 2019. I worked on setting business strategy and development, as well as on the research agenda and the engineering roadmap for 904Labs\u2019 core search and recommendation algorithms. Solumbro was an Athens-based Internet-of-Things company that designed and manufactured smart outdoor parasols. Solumbro was founded in 2015 and operated until late 2019. Solumbro was fully bootstrapped and got to a fully-working prototype , which made the news . I was an advisor at Solumbro in machine learning, big data, and software infrastructure. MyYard (WasteLogics since 2013) is a U.K.-based software company that offers entreprise resource planning (ERP) suite for waste management. I co-founded MyYard in 2005, and I was involved in designing and engineering the MyYard software until 2007. MyYard was the first in its kind to be offered in the cloud with a pay-as-you-go business model\u2013business properties that still keep it ahead of the curve. Research \u00b6 My research spans a broad spectrum of information access challenges, from predictive analytics to entity linking, online learning to rank, and voice search. Over the years, I\u2019ve explored these topics at the University of Amsterdam, 904Labs, and Apple, resulting in 55+ publications and an H-index of 20. I\u2019ve also contributed to the academic community as a program committee member for top-tier conferences and journals, and supervised numerous Ph.D., M.Sc., and B.Sc. theses, with some earning national and international recognition. Community engagement and academic collaboration have been pivotal, with active roles in conferences like SIGIR, WWW, and WSDM, among others. My Ph.D. thesis, Mining Social Media: Tracking Content and Predicting Behavior, delves into tracking news content in social media and modeling user behavior. This work, alongside my industry research, continues to shape my understanding and application of machine learning and information retrieval to solve real-world problems. Research Interests \u00b6 I have worked on a wide spectrum of information access problems ranging from predictive analytics and content tracking to entity linking and online learning to rank. Most of my research has been conducted during 2007\u20132014 at Information and Language Processing Systems (ILPS) at the University of Amsterdam and it continues today at Apple. In the table below, I list an overview of my research interests over the years: Time span Research topic 2024\u2013 Apple Intelligence, evaluation, training 2020\u20132024 Voice search, voice editing, evaluation 2014\u20132019 Product search, online learning to rank, evaluation 2012\u20132014 Entity linking, summarization, recommender systems 2008\u20132012 Predictive analytics, social media, content tracking 2007\u20132008 Speech recognition in user generated content As of September 6, 2021, I have published 55 papers with a total of 1,829 citations; my H-index is 20. You can access my publications via my Google Scholar profile . Member of Ph.D. Examining Committees \u00b6 Year My Role Ph.D. candidate Research topic 2021 Examiner Seyyed Hadi Hashemi Modeling Users Interacting with Smart Devices 2017 Copromotor David Graus Entities of Interest 2014 Examiner Damiano Spina Valenti Entity-based Filtering and Topic Detection for Online Reputation Monitoring in Twitter Student Supervision \u00b6 I am happy to have worked with more than a dozen of Ph.D., M.Sc., and B.Sc. students on a variety of research topics for their theses. Some of these masters theses have made it to a publication, some have won national thesis awards and some others have won best paper/poster awards. Year Student and research topic 2021 M.Sc. Param Popat on application-specific language models 2020 M.Sc. Sashank Gondala on error-driven pruning of language models B.Sc. Sahas Dendukuri on acoustic embeddings 2017 Ph.D. David Graus on Entities of Interest 2014 B.Sc. Thijs van der Velden on real-time music charts on Twitter M.Sc. Bart Eijk on recommendation ensembles for music discovery M.Sc. Varvara Tzika on retrieval of classifieds in auction sites M.Sc. Guido van Bruggen on analyzing real-time context of TV broadcasts M.Sc. Selvi Ratnasingam on information diffusion across languages in news and social media M.Sc. Nikos Voskarides on learning entity relations in big data 2013 M.Sc. Kerim Meijer on evaluating performance of distributed search systems M.Sc. Andrei Oghina on recommending content on news sites M.Sc. Mathias Breuss on recommending content on Twitter 2012 M.Sc. Mark Bakker on predicting movie Awards using Twitter B.Sc. Gijs van der Voort on classifying tweets as reviews 2011 B.Sc. Philo Kamenade on information propagation in Twitter 2010 M.Sc. Kamran Massoudi on microblog retrieval Ph.D. Thesis \u00b6 Mining Social Media: Tracking Content and Predicting Behavior. ( PDF, 3.7MB ) From the back cover of my thesis, Mining Social Media: Tracking Content and Predicting Behavior : The advent of social media has established a symbiotic relationship between social media and online news. This relationship can be leveraged for tracking news content, and predicting behavior with tangible real-world applications, e.g., online reputation management, ad pricing, news ranking, and media analysis. In this thesis we focus on tracking news content in social media, and predicting user behavior. In the first part, we develop methods for tracking content which build upon, and extend practices in Information Retrieval. We begin with discovering social media posts that discuss a news article yet they do not provide a hyperlink to it. Our methods model news articles using several channels of information, either endogenous or exogenous to the article. These models are then used to query an index of social media posts. During this process we found that the query models are close in size to the documents to be retrieved, violating a standard assumption of language modeling. We correct for this discrepancy by introducing two hypergeometric language models for modeling both queries, and documents to be retrieved. In the second part, we focus on predicting behavior. First we look at predicting listeners\u2019 preference in spoken user generated content, namely, podcasts. Then, we predict popularity of news articles from several news agents in terms of the volume of comments they receive. We develop models for predicting the popularity of an article for both before and after it is published. Finally, we look at a different aspect of news impact: how reading a news article affects future user browsing behavior. In each setting, we find patterns that characterize the underlying behavior and extract features that we then use to establish models for predicting online behavior. I defended my Ph.D. thesis in December 2012. I worked on it at the University of Amsterdam, under the supervision of Prof.Dr.Maarten de Rijke. Maarten has been a great supervisor and exemplary researcher. Without his guidance and the people from ILPS and my co-authors, this thesis would not have been possible. Thank you! Download a copy (PDF, 3.7MB) Community Service \u00b6 I have served in the program committee member of the following venues. Year Venue 2021 Special Interest Group on Information Retrieval (SIGIR) International Conference on Information and Knowledge Management (CIKM) SIGIR Workshop on eCommerce (SIGIReCom) Multimedia Systems Journal (MMSJ) Journal of Information Retrieval (IRJ) 2020 Web Search and Data Mining (WSDM) SIGIR Workshop on eCommerce (SIGIReCom) Transactions on Information Systems (TOIS) Special Interest Group on Information Retrieval (SIGIR) European Conference on Informantion Retrieval (ECIR) 2019 Special Interest Group on Information Retrieval (SIGIR) SIGIR Workshop on eCommerce (SIGIReCom) Transactions on Information Systems (TOIS) Word Wide Web conference (WWW) Web Search and Data Mining (WSDM) \u2013 Outstanding PC Member Award, Session chair on Graphs 2018 Special Interest Group on Information Retrieval (SIGIR) The Dutch-Belgian Information Retrieval Workshop (DIR) Word Wide Web conference (WWW) 2017 Special Interest Group on Information Retrieval (SIGIR) Transactions on Information Systems (TOIS) 2016 Special Interest Group on Information Retrieval (SIGIR) Transactions on Information Systems (TOIS) 2015 International Conference on Knowledge Discovery and Information Retrieval (KDIR) International Conference on Information and Knowledge Management (CIKM) Information Processing & Management (IPM) Web Search and Data Mining (WSDM) Artificial Intelligence (AIRE) Special Interest Group on Information Retrieval (SIGIR) Transactions on Information Systems (TOIS) 2014 Workshop on Machine Learning for Predictive Models Information Processing & Management (IPM) Special Interest Group on Information Retrieval (SIGIR) Information Retrieval Facility Conference (IRFC) Workshop on Social Multimedia and Storytelling Neurocomputing Journal Information Retrieval Journal (IRJ) Social News On the Web IEEE\u2019s Transactions on Knowledge and Data Engineering European Conference on Informantion Retrieval (ECIR) 2013 Journal of Internet Services and Applications International Joint Conference on Natural Language Processing International Workshop and Challenge on News Recommender Systems Information Retrieval Facility Conference (IRFC) Information Processing & Management (IPM) Special Interest Group on Information Retrieval (SIGIR) The Dutch-Belgian Information Retrieval Workshop (DIR) Transactions on Information Systems (TOIS) European Conference on Informantion Retrieval (ECIR) Transactions on the Web (ToW) Journal of the Association for Information Science and Technology (JASIST) 2012 The Dutch-Belgian Information Retrieval Workshop European Conference on Informantion Retrieval (ECIR) 2011 Journal of Computer Assisted Learning (JCAL) 2009 Symposium on String Processing and Information Retrieval (SPIRE)","title":"About"},{"location":"about/#highlights","text":"","title":"Highlights"},{"location":"about/#ventures","text":"904Labs was an Amsterdam-based artificial intelligence company that offers the first commercially available self-learning search engine. I co-founded 904Labs in 2014 and led the company until late 2019. I worked on setting business strategy and development, as well as on the research agenda and the engineering roadmap for 904Labs\u2019 core search and recommendation algorithms. Solumbro was an Athens-based Internet-of-Things company that designed and manufactured smart outdoor parasols. Solumbro was founded in 2015 and operated until late 2019. Solumbro was fully bootstrapped and got to a fully-working prototype , which made the news . I was an advisor at Solumbro in machine learning, big data, and software infrastructure. MyYard (WasteLogics since 2013) is a U.K.-based software company that offers entreprise resource planning (ERP) suite for waste management. I co-founded MyYard in 2005, and I was involved in designing and engineering the MyYard software until 2007. MyYard was the first in its kind to be offered in the cloud with a pay-as-you-go business model\u2013business properties that still keep it ahead of the curve.","title":"Ventures"},{"location":"about/#research","text":"My research spans a broad spectrum of information access challenges, from predictive analytics to entity linking, online learning to rank, and voice search. Over the years, I\u2019ve explored these topics at the University of Amsterdam, 904Labs, and Apple, resulting in 55+ publications and an H-index of 20. I\u2019ve also contributed to the academic community as a program committee member for top-tier conferences and journals, and supervised numerous Ph.D., M.Sc., and B.Sc. theses, with some earning national and international recognition. Community engagement and academic collaboration have been pivotal, with active roles in conferences like SIGIR, WWW, and WSDM, among others. My Ph.D. thesis, Mining Social Media: Tracking Content and Predicting Behavior, delves into tracking news content in social media and modeling user behavior. This work, alongside my industry research, continues to shape my understanding and application of machine learning and information retrieval to solve real-world problems.","title":"Research"},{"location":"posts/20080620-sigir2008-speech-termclouds/","text":"Abstract \u00b6 User generated spoken audio remains a challenge for Automatic Speech Recognition (ASR) technology and content-based audio surrogates derived from ASR-transcripts must be error robust. An investigation of the use of term clouds as surrogates for podcasts demonstrates that ASR term clouds closely approximate term clouds derived from human-generated transcripts across a range of cloud sizes. A user study confirms the conclusion that ASR-clouds are viable surrogates for depicting the content of podcasts. References \u00b6 [1] Manos Tsagkias, Martha Larson, and Maarten de Rijke. 2008. Term clouds as surrogates for user generated speech . In Proceedings of the 31 st annual international ACM SIGIR conference on Research and development in information retrieval (SIGIR \u201808). Association for Computing Machinery, New York, NY, USA, 773\u2013774. ACM Link PDF","title":"Paper at SIGIR 2008 \u2014 Term Clouds as Surrogates for User Generated Speech"},{"location":"posts/20080720-sscs2008-speech-termclouds/","text":"Abstract \u00b6 Spoken audio, like any time-continuous medium, is notoriously difficult to browse or skim without support of an interface providing semantically annotated jump points to signal the user where to listen in. Creation of time-aligned metadata by human annotators is prohibitively expensive, motivating the investigation of representations of segment-level semantic content based on transcripts generated by automatic speech recognition (ASR). This paper examines the feasibility of using term clouds to provide users with a structured representation of the semantic content of podcast episodes. Podcast episodes are visualized as a series of sub-episode segments, each represented by a term cloud derived from a transcript generated by automatic speech recognition (ASR). Quality of segment-level term clouds is measured quantitatively and their utility is investigated using a small-scale user study based on human labeled segment boundaries. Since the segment-level clouds generated from ASR-transcripts prove useful, we examine an adaptation of text tiling techniques to speech in order to be able to generate segments as part of a completely automated indexing and structuring system for browsing of spoken audio. Results demonstrate that the segments generated are comparable with human selected segment boundaries. References \u00b6 [1] Marguerite Fuller, Manos Tsagkias, Eamonn Newman, Jana Besser, Martha Larson, Gareth J.F. Jones, and Maarten de Rijke. 2008. Using Term Clouds to Represent Segment-Level Semantic Content of Podcasts . In Proceedings of the 2 nd SIGIR Workshop on Searching Spontaneous Conversational Speech (SSCS 2008). UvA Link PDF","title":"Paper at SSCS 2008 \u2014\u00a0Using Term Clouds to Represent Segment-Level Semantic Content of Podcasts"},{"location":"posts/20081120-wicow2008-podcred/","text":"Abstract \u00b6 The PodCred framework is a framework for assessing the credibility and quality of podcasts published on the internet. It consists of a series of indicators designed to support prediction of listener preference of one podcast over another, given that both carry comparable informational content. The indicators are grouped into four categories pertaining to the Podcast Content, the Podcaster, the Podcast Context or the Technical Execution of the podcast. We adopt the term \u201ccred\u201d as a designation encompassing both credibility (comprising trustworthiness and expertise) and qualitative acceptability to listeners. Our podcast analysis framework is inspired by work on credibility in blogs, another medium dominated by user generated content. The PodCred framework is derived from a review of the literature on credibility for other media, a survey of prescriptive standards for podcasting, and a detailed data analysis of award winning podcasts. The paper concludes with a discussion of future work in which the framework will be applied. References \u00b6 [1] Manos Tsagkias, Martha Larson, Wouter Weerkamp, and Maarten de Rijke. 2008. PodCred: a framework for analyzing podcast preference. In Proceedings of the 2 nd ACM Workshop on Information Credibility On the Web (WICOW \u201808). Association for Computing Machinery, New York, NY, USA, 67\u201374. ACM link PDF","title":"Paper at WICOW 2008 \u2014\u00a0PodCred: A Framework for Analyzing Podcast Preference"},{"location":"posts/20081202-siren2008-talk-speech-termclouds/","text":"I took part in SIREN 2008, a research event in the Netherlands, presenting our work with Martha Larson and Maarten de Rijke, on Term Clouds as Surrogates for User Generated Speech [ 1 ]; see post References \u00b6 [1] Manos Tsagkias, Martha Larson, and Maarten de Rijke. 2008. Term clouds as surrogates for user generated speech . In Proceedings of the 31 st annual international ACM SIGIR conference on Research and development in information retrieval (SIGIR \u201808). Association for Computing Machinery, New York, NY, USA, 773\u2013774. ACM Link PDF","title":"Talk at SIREN 2008 on Speech Term Clouds"},{"location":"posts/20090221-www2009-weps/","text":"Abstract \u00b6 In this paper we describe our participation [ 1 ] in the Second Web People Search workshop (WePS2) and detail our approaches. For the clustering task, our focus was on replicating the lessons learned at WEPS1 on the data set made available as part of WEPS2 and on experimenting with a voting-based combination of clustering methods. We found that clustering methods display the same overall behavior on the WEPS1 and WESP2 data sets and that a hierarchical clustering approach delivers the best performance, even outperforming voting-based combinations. For attribute extraction, we explore approaches using pattern matching with manually and automatically constructed patterns. Manual patterns were constructed using expert knowledge and following analysis of sample data. Automatic pattern construction extracts textual and syntactic context around training samples and selects patterns which are expected to perform well based on leave-one-out evaluation. Experimental results show that manually constructed patterns are very e\ufb00ective for obtaining high recall. For automatically extracted patterns performance varied widely depending on the attribute type. Larger amounts of training data may help improve these approaches in the future. References \u00b6 [1] Krisztian Balog, Jiyin He, Katja Hofmann, Valentin Jijkoun, Christof Monz, Manos Tsagkias, Wouter Weerkamp, and Maarten de Rijke. The University of Amsterdam at WePS2. In 2 nd Web People Search Evaluation Workshop (WePS 2009) . 18 th WWW Conference, March 2009. PDF .","title":"Paper at WWW 2009 \u2014\u00a0The University of Amsterdam at Web People Search Benchmark (WePS)"},{"location":"posts/20090304-ecir2009-podcred-features/","text":"Abstract \u00b6 Podcasts display an unevenness characteristic of domains dominated by user generated content, resulting in potentially radical variation of the user preference they enjoy. We report on work that uses easily extractable surface features of podcasts in order to achieve solid performance on two podcast preference prediction tasks: classification of preferred vs. non-preferred podcasts and ranking podcasts by level of preference. We identify features with good discriminative potential by carrying out manual data analysis, resulting in a refinement of the indicators of an existent podcast preference framework. Our preference prediction is useful for topic-independent ranking of podcasts, and can be used to support download suggestion or collection browsing. References \u00b6 [1] Manos Tsagkias, Martha Larson, and Maarten Rijke. 2009. Exploiting Surface Features for the Prediction of Podcast Preference. In Proceedings of the 31th European Conference on IR Research on Advances in Information Retrieval (ECIR \u201809). Springer-Verlag, Berlin, Heidelberg, 473\u2013484. ACM Link PDF","title":"Paper at ECIR 2009 \u2014\u00a0Exploiting Surface Features for the Prediction of Podcast Preference"},{"location":"posts/20090903-cikm2009-predicting-comments/","text":"Abstract \u00b6 On-line news agents provide commenting facilities for readers to express their views with regard to news stories. The number of user supplied comments on a news article may be indicative of its importance or impact. We report on exploratory work that predicts the comment volume of news articles prior to publication using five feature sets. We address the prediction task as a two stage classification task: a binary classification identifies articles with the potential to receive comments, and a second binary classification receives the output from the first step to label articles \u201clow\u201d or \u201chigh\u201d comment volume. The results show solid performance for the former task, while performance degrades for the latter. References \u00b6 [1] Manos Tsagkias, Wouter Weerkamp, and Maarten de Rijke. 2009. Predicting the volume of comments on online news stories . In Proceedings of the 18 th ACM conference on Information and knowledge management (CIKM \u201809). Association for Computing Machinery, New York, NY, USA, 1765\u20131768. ACM Link PDF","title":"Paper at CIKM 2009 \u2014\u00a0Predicting the Volume of Comments on Online News Stories"},{"location":"posts/20091026-trec2009/","text":"Abstract \u00b6 We describe the participation of the University of Amsterdam\u2019s ILPS group in the web, blog, web, entity, and relevance feedback track at TREC 2009. Our main preliminary conclusions are as follows. For the Blog track we find that for top stories identification a blogs to news approach outperforms a simple news to blogs approach. This is interesting, as this approach starts with no input except for a date, whereas the news to blogs approach also has news headlines as input. For the web track, we find that spam is an important issue in the ad hoc task and that Wikipedia- based heuristic optimization approaches help to boost the retrieval performance, which is assumed to potentially reduce the spam in top ranked documents. As for the diversity task, we explored different methods. Initial results show that clustering and a topic model-based approach have similar performance, which are relatively better than a query log based approach. Our performance in the Entity track was downright disappointing; the use of co-occurrence models led to poor results; an initial analysis shows that while our approach is able to find correct entity names, we fail to find homepages for these entities. For the relevance feedback track we find that a topical diversity approach provides good feedback documents. Further, we find that our relevance feedback algorithm seems to help most when there are sufficient relevant documents available. References \u00b6 [1] Krisztian Balog, Marc Bron, Jiyin He, Katja Hofmann, Edgar Meij, Maarten de Rijke, Tsagkias, and Wouter Weerkamp. The University of Amsterdam at TREC 2009: Blog, Web, Entity and Relevance Feedback . In TREC 2009 Working Notes. NIST, November 2009. PDF","title":"Paper at TREC 2009 \u2014\u00a0The University of Amsterdam at TREC 2009"},{"location":"posts/20101002-kl-divergence/","text":"6 December 2024\u2014Note: This post is a restoration of an older post, which is no longer available online. Content-wise this version is identical to the original. The original version can still be found in the WayBack Machine . Content Working with documents Symmetric KL-divergence Over which random variables? Simple back-off The code Let \\(P\\) and \\(Q\\) be two probability distributions of a discrete random variable. If the following two properties hold: when \\(P\\) and \\(Q\\) both sum to \\(1\\) and for any \\(i\\) such that \\(P(i) > 0\\) and \\(Q(i) > 0\\) then, we can define their KL-divergence as: \\[D_{KL}(P||Q) = \\sum_{i}P(i)log\\frac{P(i)}{Q(i)},\\] and it has three properties: \\(D_{KL}(P||Q) \\neq D_{KL}(Q||P)\\) (asymmetry) it is additive for independent distributions \\(D_{KL} \\geq 0\\) with \\(D_{KL} = 0\\) iff \\(P=Q\\) Working with documents \u00b6 We regard a document \\(d\\) as discrete distribution of \\(|d|\\) random variables, where \\(|d|\\) is the number of words in the document. Now, let \\(d_{1}\\) and \\(d_{2}\\) be two documents for which we want to calculate their KL-divergence. We run into two problems: we need to compute the KL-divergence twice due to asymmetry: \\(D_{KL}(d_{1}||d_{2})\\) and \\(D_{KL}(d_{2}||d_{1})\\) . also, due to the 2 nd constraint for defining KL-divergence, our calculations should only consider words occurring in both \\(d_{1}\\) and \\(d_{2}\\) . Symmetric KL-divergence \u00b6 We start from the 2 nd property of KL-divergence: \\[\\begin{array}{rcl} D_{KL}(P||Q) + D_{KL}(Q||P) & = & \\sum_{i}P(i)log\\frac{P(i)}{Q(i)} + \\sum_{i}Q(i)log\\frac{Q(i)}{P(i)} \\\\& = & \\sum_{i}P(i)log\\frac{P(i)}{Q(i)}+Q(i)log\\frac{Q(i)}{P(i)}\\\\ & = & \\sum_{i}P(i)log\\frac{P(i)}{Q(i)}-Q(i)log\\frac{P(i)}{Q(i)}\\\\ & = & \\sum_{i}(P(i)-Q(i))log\\frac{P(i)}{Q(i)}\\end{array}\\] Ok! It looks good [ 1 ]! Now we need to compute KL-divergence only once for every pair of documents. Over which random variables? \u00b6 Now let\u2019s turn into how to handle documents with no or little overlapping vocabularies. To illustrate the problem, consider the following documents: d1 : This is a document d2 : This is a sentence For each document we remove stopwords (\u2018this\u2019, \u2018is\u2019, \u2018a\u2019) so they become: d1 : document d2 : sentence According to constraint 2, we need to operate on the intersection of the documents\u2019 vocabularies: \\(d_{1}\\cap d_{2}=\\emptyset\\) . We end up with the empty set and therefore we cannot compute directly the KL-divergence. In this case we can assign it a large number like \\(1e33\\) . Let\u2019s see what happens when we have larger documents. d1 : Many research publications want you to use BibTeX which better organizes the whole process. Suppose for concreteness your source file is x.tex. Basically, you create a file x.bib containing the bibliography, and run bibtex on that file. d2 : In this case you must supply both a \\left and a \\right because the delimiter height are made to match whatever is contained between the two commands. But, the \\left doesn\u2019t have to be an actual \u2018left delimiter\u2019, that is you can use \u2018\\left)\u2019 if there were some reason to do it. After stopword removal, lowercasing and discarding words less than \\(2\\) characters, the documents become: d1 : many research publications want you use bibtex better organizes whole process suppose concreteness your source file tex basically you create file bib containing bibliography run bibtex file d2 : case you must supply both left right because delimiter height made match whatever contained between two commands left doesn have actual left delimiter you use left some reason The vocabulary intersection of the documents consists of two terms: \u201cuse\u201d and \u201cyou\u201d. In \\(d_{1}\\) \u201cuse\u201d occurs 1 time and \u201cyou\u201d occurs 2 times. Surprisingly, in \\(d_{2}\\) \u201cuse\u201d also occurs \\(1\\) time and \u201cyou\u201d occurs \\(2\\) times too. The distributions \\(D_{1}\\) and \\(D_{2}\\) are equal, and therefore \\(D_{KLsym}(D_{1}||D_{2}) = 0\\) . So these documents are deemed equal! A better stopword list could have removed \u201cuse\u201d and \u201cyou\u201d and in that case the documents would have an infinite KL-divergence as in the first example. However it is easy to think of similar examples where stopword lists wouldn\u2019t have been of much help. So, how can we overcome this problem? Simple back-off \u00b6 Since operating on the vocabulary intersection is not an option, we need to find a trick that allows us to consider the entire vocabulary of the documents. Smoothing comes to mind. Dirichlet and Laplacian smoothing are amongst the most popular smoothing techniques but after smoothing the probability distribution doesn\u2019t sum up to \\(1\\) and violates the first constraint for defining KL-divergence. Brigette Biggi [ 2 ] suggested a back off smoothing method which keeps the probability distributions sums to \\(1\\) and also allows operating on the entire vocabulary. According to their proposed back-off method, the smoothed probability \\(P'(t, d)\\) of a term \\(t\\) in a document \\(d\\) is: \\[P'(t_{i},d) = \\left\\{ \\begin{array}{ll} \\gamma P(t_{i}|d) & \\quad \\text{if it occurs in d}\\\\ \\epsilon & \\quad \\text{otherwise}\\\\ \\end{array} \\right.\\] where \\[P(t_{i}|d) = \\frac{tf(t_{i}, d)}{\\sum_{x\\in d}tf(t_{x},d)}\\] the interesting part is on how \\(\\gamma\\) and \\(\\epsilon\\) are calculated. In order to keep the term probabilities for \\(d_{1}\\) and \\(d_{2}\\) summing up to \\(1\\) , the following constraint should be met: \\[\\sum_{i \\in d}\\gamma P(t_{i}|d) + \\sum_{i \\in d_{1}, i \\notin d_{2}}\\epsilon = 1\\] where \\(\\gamma\\) is a normalization coefficient: \\[\\gamma = 1 - \\sum_{i \\in d_{1}, i \\notin d_{2}}\\epsilon\\] and \\(\\epsilon\\) is a positive number smaller than the minimum term probability occurring in either \\(d_{1}\\) or \\(d_{2}\\) . The code \u00b6 To illustrate the above, I wrote a small Python script: import re , math , collections def tokenize (_str): stopwords = [ 'and' , 'for' , 'if' , 'the' , 'then' , 'be' , 'is' , 'are' , 'will' , 'in' , 'it' , 'to' , 'that' ] tokens = collections . defaultdict( lambda : 0. ) for m in re . finditer( r\"(\\w+)\" , _str, re . UNICODE): m = m . group( 1 ) . lower() if len (m) < 2 : continue if m in stopwords: continue tokens[m] += 1 return tokens #end of tokenize def kldiv (_s, _t): if ( len (_s) == 0 ): return 1e33 if ( len (_t) == 0 ): return 1e33 ssum = 0. + sum (_s . values()) slen = len (_s) tsum = 0. + sum (_t . values()) tlen = len (_t) vocabdiff = set (_s . keys()) . difference( set (_t . keys())) lenvocabdiff = len (vocabdiff) \"\"\" epsilon \"\"\" epsilon = min ( min (_s . values()) / ssum, min (_t . values()) / tsum) * 0.001 \"\"\" gamma \"\"\" gamma = 1 - lenvocabdiff * epsilon # print \"_s: %s\" % _s # print \"_t: %s\" % _t \"\"\" Check if distribution probabilities sum to 1\"\"\" sc = sum ([v / ssum for v in _s . itervalues()]) st = sum ([v / tsum for v in _t . itervalues()]) if sc < 9e-6 : print \"Sum P: %e , Sum Q: %e \" % (sc, st) print \"*** ERROR: sc does not sum up to 1. Bailing out ..\" sys . exit( 2 ) if st < 9e-6 : print \"Sum P: %e , Sum Q: %e \" % (sc, st) print \"*** ERROR: st does not sum up to 1. Bailing out ..\" sys . exit( 2 ) div = 0. for t, v in _s . iteritems(): pts = v / ssum ptt = epsilon if t in _t: ptt = gamma * (_t[t] / tsum) ckl = (pts - ptt) * math . log(pts / ptt) div += ckl return div #end of kldiv d1 = \"\"\"Many research publications want you to use BibTeX, which better organizes the whole process. Suppose for concreteness your source file is x.tex. Basically, you create a file x.bib containing the bibliography, and run bibtex on that file.\"\"\" d2 = \"\"\"In this case you must supply both a \\left and a \\r ight because the delimiter height are made to match whatever is contained between the two commands. But, the \\left doesn't have to be an actual 'left delimiter', that is you can use '\\left)' if there were some reason to do it.\"\"\" print \"KL-divergence between d1 and d2:\" , kldiv(tokenize(d1), tokenize(d2)) print \"KL-divergence between d2 and d1:\" , kldiv(tokenize(d2), tokenize(d1)) The output looks like this: KL-divergence between d1 and d2: 6.52185430964 KL-divergence between d2 and d1: 6.51142363095 Now, KL-divergence is greater than zero, so the documents are not thought to be the same as before! Good job. Looking at KL symmetry, although the divergence of the two pairs is not identical, it is sufficiently close [ 1 ]. Acknowledgments \u00b6 The above is a compilation of knowledge found in Wikipedia and from the paper \u201cReducing the Plagiarism Detection Search Space on the basis of Kullback-Leibler Distance\u201d by Alberto Barron-Cedeno, Paolo Rosso, and Jose-Miguel Benedi [ 1 ]. Examples and code are mine and you are free to use them. References \u00b6 [1] Pinto, D., Bened\u00ed, JM., Rosso, P. (2007). Clustering Narrow-Domain Short Texts by Using the Kullback-Leibler Distance . In Computational Linguistics and Intelligent Text Processing (CICLing 2007). Lecture Notes in Computer Science, vol 4394. Springer, Berlin, Heidelberg. Link [2] Bigi, B. (2003). Using Kullback-Leibler Distance for Text Categorization . In Advances in Information Retrieval (ECIR 2003). Lecture Notes in Computer Science, vol 2633. Springer, Berlin, Heidelberg. Link","title":"KL-divergence of Two Documents"},{"location":"posts/20101002-kl-divergence/#working-with-documents","text":"We regard a document \\(d\\) as discrete distribution of \\(|d|\\) random variables, where \\(|d|\\) is the number of words in the document. Now, let \\(d_{1}\\) and \\(d_{2}\\) be two documents for which we want to calculate their KL-divergence. We run into two problems: we need to compute the KL-divergence twice due to asymmetry: \\(D_{KL}(d_{1}||d_{2})\\) and \\(D_{KL}(d_{2}||d_{1})\\) . also, due to the 2 nd constraint for defining KL-divergence, our calculations should only consider words occurring in both \\(d_{1}\\) and \\(d_{2}\\) .","title":"Working with documents"},{"location":"posts/20101002-kl-divergence/#symmetric-kl-divergence","text":"We start from the 2 nd property of KL-divergence: \\[\\begin{array}{rcl} D_{KL}(P||Q) + D_{KL}(Q||P) & = & \\sum_{i}P(i)log\\frac{P(i)}{Q(i)} + \\sum_{i}Q(i)log\\frac{Q(i)}{P(i)} \\\\& = & \\sum_{i}P(i)log\\frac{P(i)}{Q(i)}+Q(i)log\\frac{Q(i)}{P(i)}\\\\ & = & \\sum_{i}P(i)log\\frac{P(i)}{Q(i)}-Q(i)log\\frac{P(i)}{Q(i)}\\\\ & = & \\sum_{i}(P(i)-Q(i))log\\frac{P(i)}{Q(i)}\\end{array}\\] Ok! It looks good [ 1 ]! Now we need to compute KL-divergence only once for every pair of documents.","title":"Symmetric KL-divergence"},{"location":"posts/20101002-kl-divergence/#over-which-random-variables","text":"Now let\u2019s turn into how to handle documents with no or little overlapping vocabularies. To illustrate the problem, consider the following documents: d1 : This is a document d2 : This is a sentence For each document we remove stopwords (\u2018this\u2019, \u2018is\u2019, \u2018a\u2019) so they become: d1 : document d2 : sentence According to constraint 2, we need to operate on the intersection of the documents\u2019 vocabularies: \\(d_{1}\\cap d_{2}=\\emptyset\\) . We end up with the empty set and therefore we cannot compute directly the KL-divergence. In this case we can assign it a large number like \\(1e33\\) . Let\u2019s see what happens when we have larger documents. d1 : Many research publications want you to use BibTeX which better organizes the whole process. Suppose for concreteness your source file is x.tex. Basically, you create a file x.bib containing the bibliography, and run bibtex on that file. d2 : In this case you must supply both a \\left and a \\right because the delimiter height are made to match whatever is contained between the two commands. But, the \\left doesn\u2019t have to be an actual \u2018left delimiter\u2019, that is you can use \u2018\\left)\u2019 if there were some reason to do it. After stopword removal, lowercasing and discarding words less than \\(2\\) characters, the documents become: d1 : many research publications want you use bibtex better organizes whole process suppose concreteness your source file tex basically you create file bib containing bibliography run bibtex file d2 : case you must supply both left right because delimiter height made match whatever contained between two commands left doesn have actual left delimiter you use left some reason The vocabulary intersection of the documents consists of two terms: \u201cuse\u201d and \u201cyou\u201d. In \\(d_{1}\\) \u201cuse\u201d occurs 1 time and \u201cyou\u201d occurs 2 times. Surprisingly, in \\(d_{2}\\) \u201cuse\u201d also occurs \\(1\\) time and \u201cyou\u201d occurs \\(2\\) times too. The distributions \\(D_{1}\\) and \\(D_{2}\\) are equal, and therefore \\(D_{KLsym}(D_{1}||D_{2}) = 0\\) . So these documents are deemed equal! A better stopword list could have removed \u201cuse\u201d and \u201cyou\u201d and in that case the documents would have an infinite KL-divergence as in the first example. However it is easy to think of similar examples where stopword lists wouldn\u2019t have been of much help. So, how can we overcome this problem?","title":"Over which random variables?"},{"location":"posts/20101002-kl-divergence/#simple-back-off","text":"Since operating on the vocabulary intersection is not an option, we need to find a trick that allows us to consider the entire vocabulary of the documents. Smoothing comes to mind. Dirichlet and Laplacian smoothing are amongst the most popular smoothing techniques but after smoothing the probability distribution doesn\u2019t sum up to \\(1\\) and violates the first constraint for defining KL-divergence. Brigette Biggi [ 2 ] suggested a back off smoothing method which keeps the probability distributions sums to \\(1\\) and also allows operating on the entire vocabulary. According to their proposed back-off method, the smoothed probability \\(P'(t, d)\\) of a term \\(t\\) in a document \\(d\\) is: \\[P'(t_{i},d) = \\left\\{ \\begin{array}{ll} \\gamma P(t_{i}|d) & \\quad \\text{if it occurs in d}\\\\ \\epsilon & \\quad \\text{otherwise}\\\\ \\end{array} \\right.\\] where \\[P(t_{i}|d) = \\frac{tf(t_{i}, d)}{\\sum_{x\\in d}tf(t_{x},d)}\\] the interesting part is on how \\(\\gamma\\) and \\(\\epsilon\\) are calculated. In order to keep the term probabilities for \\(d_{1}\\) and \\(d_{2}\\) summing up to \\(1\\) , the following constraint should be met: \\[\\sum_{i \\in d}\\gamma P(t_{i}|d) + \\sum_{i \\in d_{1}, i \\notin d_{2}}\\epsilon = 1\\] where \\(\\gamma\\) is a normalization coefficient: \\[\\gamma = 1 - \\sum_{i \\in d_{1}, i \\notin d_{2}}\\epsilon\\] and \\(\\epsilon\\) is a positive number smaller than the minimum term probability occurring in either \\(d_{1}\\) or \\(d_{2}\\) .","title":"Simple back-off"},{"location":"posts/20101002-kl-divergence/#the-code","text":"To illustrate the above, I wrote a small Python script: import re , math , collections def tokenize (_str): stopwords = [ 'and' , 'for' , 'if' , 'the' , 'then' , 'be' , 'is' , 'are' , 'will' , 'in' , 'it' , 'to' , 'that' ] tokens = collections . defaultdict( lambda : 0. ) for m in re . finditer( r\"(\\w+)\" , _str, re . UNICODE): m = m . group( 1 ) . lower() if len (m) < 2 : continue if m in stopwords: continue tokens[m] += 1 return tokens #end of tokenize def kldiv (_s, _t): if ( len (_s) == 0 ): return 1e33 if ( len (_t) == 0 ): return 1e33 ssum = 0. + sum (_s . values()) slen = len (_s) tsum = 0. + sum (_t . values()) tlen = len (_t) vocabdiff = set (_s . keys()) . difference( set (_t . keys())) lenvocabdiff = len (vocabdiff) \"\"\" epsilon \"\"\" epsilon = min ( min (_s . values()) / ssum, min (_t . values()) / tsum) * 0.001 \"\"\" gamma \"\"\" gamma = 1 - lenvocabdiff * epsilon # print \"_s: %s\" % _s # print \"_t: %s\" % _t \"\"\" Check if distribution probabilities sum to 1\"\"\" sc = sum ([v / ssum for v in _s . itervalues()]) st = sum ([v / tsum for v in _t . itervalues()]) if sc < 9e-6 : print \"Sum P: %e , Sum Q: %e \" % (sc, st) print \"*** ERROR: sc does not sum up to 1. Bailing out ..\" sys . exit( 2 ) if st < 9e-6 : print \"Sum P: %e , Sum Q: %e \" % (sc, st) print \"*** ERROR: st does not sum up to 1. Bailing out ..\" sys . exit( 2 ) div = 0. for t, v in _s . iteritems(): pts = v / ssum ptt = epsilon if t in _t: ptt = gamma * (_t[t] / tsum) ckl = (pts - ptt) * math . log(pts / ptt) div += ckl return div #end of kldiv d1 = \"\"\"Many research publications want you to use BibTeX, which better organizes the whole process. Suppose for concreteness your source file is x.tex. Basically, you create a file x.bib containing the bibliography, and run bibtex on that file.\"\"\" d2 = \"\"\"In this case you must supply both a \\left and a \\r ight because the delimiter height are made to match whatever is contained between the two commands. But, the \\left doesn't have to be an actual 'left delimiter', that is you can use '\\left)' if there were some reason to do it.\"\"\" print \"KL-divergence between d1 and d2:\" , kldiv(tokenize(d1), tokenize(d2)) print \"KL-divergence between d2 and d1:\" , kldiv(tokenize(d2), tokenize(d1)) The output looks like this: KL-divergence between d1 and d2: 6.52185430964 KL-divergence between d2 and d1: 6.51142363095 Now, KL-divergence is greater than zero, so the documents are not thought to be the same as before! Good job. Looking at KL symmetry, although the divergence of the two pairs is not identical, it is sufficiently close [ 1 ].","title":"The code"},{"location":"posts/20101008-wsdm2011-linking-online-news/","text":"Abstract \u00b6 Much of what is discussed in social media is inspired by events in the news and, vice versa, social media provide us with a handle on the impact of news events. We address the following linking task: given a news article, find social media utterances that implicitly reference it. We follow a three-step approach: we derive multiple query models from a given source news article, which are then used to retrieve utterances from a target social media index, resulting in multiple ranked lists that we then merge using data fusion techniques. Query models are created by exploiting the structure of the source article and by using explicitly linked social media utterances that discuss the source article. To combat query drift resulting from the large volume of text, either in the source news article itself or in social media utterances explicitly linked to it, we introduce a graph-based method for selecting discriminative terms. For our experimental evaluation, we use data from Twitter, Digg, Delicious, the New York Times Community, Wikipedia, and the blogosphere to generate query models. We show that different query models, based on different data sources, provide complementary information and manage to retrieve different social media utterances from our target index. As a consequence, data fusion methods manage to significantly boost retrieval performance over individual approaches. Our graph-based term selection method is shown to help improve both effectiveness and efficiency. References \u00b6 [1] Manos Tsagkias, Maarten de Rijke, and Wouter Weerkamp. 2011. Linking online news and social media . In Proceedings of the fourth ACM international conference on Web search and data mining (WSDM \u201811). Association for Computing Machinery, New York, NY, USA, 565\u2013574. ACM Link PDF","title":"Paper at WSDM 2011 \u2014\u00a0Linking online news and social media"},{"location":"posts/20110119-ecir2011-query-expansion/","text":"Abstract \u00b6 We propose a retrieval model for searching microblog posts for a given topic of interest. We develop a language modeling approach tailored to microblogging characteristics, where redundancy-based IR methods cannot be used in a straightforward manner. We enhance this model with two groups of quality indicators: textual and microblog specific. Additionally, we propose a dynamic query expansion model for microblog post retrieval. Experimental results on Twitter data reveal the usefulness of boolean search, and demonstrate the utility of quality indicators and query expansion in microblog search. References \u00b6 [1] Massoudi, K., Tsagkias, M., de Rijke, M., Weerkamp, W. (2011). Incorporating Query Expansion and Quality Indicators in Searching Microblog Posts . In Advances in Information Retrieval. ECIR 2011. Lecture Notes in Computer Science, vol 6611. Springer, Berlin, Heidelberg. Springer link ; PDF","title":"Paper at ECIR 2011 \u2014 Incorporating Query Expansion and Quality Indicators in Searching Microblog Posts"},{"location":"posts/20110120-dir2011-language-identification/","text":"Abstract \u00b6 Offering access to information in microblog posts requires successful language identification. Language identification on sparse and noisy data can be challenging. In this paper we explore the performance of a state-of-the-art n-gram-based language identifier, and we introduce two semi-supervised priors to enhance performance at microblog post level: (i) blogger-basedprior, using previous posts by the same blogger, and (ii) link-based prior, using the pages linked to from the post. We test our models on five languages (Dutch, English, French, German, and Spanish), and a set of 1,000 tweets per language. Results show that our priors improve accuracy, but that there is still room for improvement. References \u00b6 [1] Simon Carter, Manos Tsagkias, and Wouter Weerkamp. 2011. Semi-Supervised Priors for Microblog Language Identification . In DIR 2011: Dutch-Belgian Information Retrieval Workshop Amsterdam (pp. 12-15). University of Amsterdam, Information and Language Processing group. PDF","title":"Paper at DIR 2011 \u2014\u00a0Semi-Supervised Priors for Microblog Language Identification"},{"location":"posts/20110408-sigir2011-hypergeometric-language-models/","text":"Abstract \u00b6 Republished article finding is the task of identifying instances of articles that have been published in one source and republished more or less verbatim in another source, which is often a social media source. We address this task as an ad hoc retrieval problem, using the source article as a query. Our approach is based on language modeling. We revisit the assumptions underlying the unigram language model taking into account the fact that in our setup queries are as long as complete news articles. We argue that in this case, the underlying generative assumption of sampling words from a document with replacement, i.e., the multinomial modeling of documents, produces less accurate query likelihood estimates. To make up for this discrepancy, we consider distributions that emerge from sampling without replacement: the central and non-central hypergeometric distributions. We present two retrieval models that build on top of these distributions: a log odds model and a bayesian model where document parameters are estimated using the Dirichlet compound multinomial distribution. We analyse the behavior of our new models using a corpus of news articles and blog posts and find that for the task of republished article finding, where we deal with queries whose length approaches the length of the documents to be retrieved, models based on distributions associated with sampling without replacement outperform traditional models based on multinomial distributions. References \u00b6 [1] Manos Tsagkias, Maarten de Rijke, and Wouter Weerkamp. 2011. Hypergeometric language models for republished article finding . In Proceedings of the 34 th international ACM SIGIR conference on Research and development in Information Retrieval (SIGIR \u201811). Association for Computing Machinery, New York, NY, USA, 485\u2013494. ACM Link ; PDF .","title":"Paper at SIGIR 2011 \u2014\u00a0Hypergeometric Language Models for Republished Article Finding"},{"location":"posts/20110409-websci2011-hashtag-translation/","text":"Abstract \u00b6 The popularity of microblogging platforms, such as Twitter, renders them valuable real-time information resources for tracking various aspects of worldwide events, e.g., earthquakes, political elections, etc. Such events are usually characterized in microblog posts via the use of hashtags (#). As microbloggers come from different backgrounds, and express themselves in different languages, we witness different \u201ctranslations\u201d of hashtags which, however, are about the same event. Language-dependent variants of hashtags can possibly lead to issues in content-analysis. In this paper, we propose a method for translating hashtags, which builds on methods from information retrieval. The method introduced is source and target language independent. Our method is desirable, either instead of, or complimentary, to the direct translation of the hashtag for three reasons. First we return a list of hashtags on the same topic, which takes into account the plurality and variability of hashtags used by microbloggers for assigning posts to a topic. Second, our framework accounts for the problem that microbloggers in different languages will refer to the same topic using different tokens. Finally, our method does not require special preprocessing of hashtags, reducing barriers to real-world implementation. We present proof-of-concept results for the given Spanish hashtag #33mineros. References \u00b6 [1] Simon Carter, Manos Tsagkias, and Wouter Weerkamp. 2011. Twitter hashtags: Joint Translation and Clustering . In Web Science 2011. Uva Link ; PDF .","title":"Paper at WebScience 2011 \u2014\u00a0Twitter hashtags: Joint Translation and Clustering"},{"location":"posts/20110409-websci2011-twitter-use-in-languages/","text":"Abstract \u00b6 In this paper we describe how Twitter is used in various languages. We observe notable differences between languages regarding the use of hashtags, links, mentions, and conversations. We propose two dimensions that can be used to classify languages, each of which is likely to require different ways of analysis. References \u00b6 [1] Wouter Weerkamp, Simon Carter, and Manos Tsagkias. 2011. How People use Twitter in Different Languages . In Web Science 2011. UVA Link ; PDF .","title":"Paper at WebScience 2011 \u2014 How People use Twitter in Different Languages"},{"location":"posts/20110914-yahoo-research/","text":"I moved to Barcelona in the first week of September for a three-month internship at Yahoo! Research Labs. I\u2019m very excited about it, and I\u2019m looking forward to getting to know the people here and the problems they are working on. Update: My work at Yahoo! Research Labs resulted in a publication at SIGIR 2012 [ 1 ]; see [post] for the abstract(20120623-sigir2012-language-intent-models.md). References \u00b6 [1] Manos Tsagkias and Roi Blanco. 2012. Language intent models for inferring user browsing behavior . In Proceedings of the 35 th international ACM SIGIR conference on research and development in information retrieval (SIGIR \u201812). Association for Computing Machinery, New York, NY, USA, 335\u2013344. ACM Link . PDF","title":"Internship at Yahoo! Research Labs"},{"location":"posts/20120329-ecir2012-imdb/","text":"Abstract \u00b6 We predict IMDb movie ratings and consider two sets of features: surface and textual features. For the latter, we assume that no social media signal is isolated and use data from multiple channels that are linked to a particular movie, such as tweets from Twitter and comments from YouTube. We extract textual features from each channel to use in our prediction model and we explore whether data from either of these channels can help to extract a better set of textual feature for prediction. Our best performing model is able to rate movies very close to the observed values. Poster of the paper presented at ECIR 2012. Part of the Artificial Intelligence Masters programme in which I used to teach was one month hands-on project. I have supervised Andrei Oghina and Mathias Breuss on whether can we predict a movie\u2019s IMDB rating from social media before the movie is out . Andrei and Mathias collected tweets about movies and their respective YouTube trailers, and they extracted several features which were used for training a rating classifier. We have found out that we are able predict a movie\u2019s IMDB rating with \u00b10.25 accuracy. That is if a movie gets an average IMDB rating of 8, we would predict 7.75 or 8.25. This is quite impressive for predicting a movie\u2019s success before it is even released. Our work has been published at ECIR 2012 [ 1 ], and it has been awarded the best paper award; PDF (495KB) . References \u00b6 [1]: Andrei Oghina, Mathias Breuss, Manos Tsagkias, and Maarten de Rijke. Predicting IMDB Move Ratings Using Social Media . In European Conference on Information Retrieval (ECIR) 2012. ACM Library ; PDF (495KB) .","title":"Paper at ECIR 2012 \u2014 Predicting IMDB Move Ratings Using Social Media"},{"location":"posts/20120623-sigir2012-language-intent-models/","text":"Abstract \u00b6 Modeling user browsing behavior is an active research area with tangible real-world applications, e.g., organizations can adapt their online presence to their visitors browsing behavior with positive effects in user engagement, and revenue. We concentrate on online news agents, and present a semi-supervised method for predicting news articles that a user will visit after reading an initial article. Our method tackles the problem using language intent models trained on historical data which can cope with unseen articles. We evaluate our method on a large set of articles and in several experimental settings. Our results demonstrate the utility of language intent models for predicting user browsing behavior within online news sites. I am very happy that our paper Language Intent Models for Inferring User Browsing Behavior by Manos Tsagkias, and Roi Blanco [ 1 ] has been accepted at SIGIR 2013, which will be held in Portland, Oregon, 12\u201316 August 2012. The paper was realized during my three-month internship at Yahoo! Research Barcelona during September\u2013December 2011 . References \u00b6 [1] Manos Tsagkias and Roi Blanco. 2012. Language intent models for inferring user browsing behavior . In Proceedings of the 35 th international ACM SIGIR conference on research and development in information retrieval (SIGIR \u201812). Association for Computing Machinery, New York, NY, USA, 335\u2013344. ACM Link . PDF","title":"Paper at SIGIR 2012 \u2014\u00a0Language Intent Models for Inferring User Browsing Behavior"},{"location":"posts/20121108-phd-thesis-mining-social-media/","text":"From the back cover\u2014 Mining Social Media: Tracking Content and Predicting Behavior. ( PDF, 3.7MB ) The advent of social media has established a symbiotic relationship between social media and online news. This relationship can be leveraged for tracking news content, and predicting behavior with tangible real-world applications, e.g., online reputation management, ad pricing, news ranking, and media analysis. In this thesis we focus on tracking news content in social media, and predicting user behavior. In the first part, we develop methods for tracking content which build upon, and extend practices in Information Retrieval. We begin with discovering social media posts that discuss a news article yet they do not provide a hyperlink to it. Our methods model news articles using several channels of information, either endogenous or exogenous to the article. These models are then used to query an index of social media posts. During this process we found that the query models are close in size to the documents to be retrieved, violating a standard assumption of language modeling. We correct for this discrepancy by introducing two hypergeometric language models for modeling both queries, and documents to be retrieved. In the second part, we focus on predicting behavior. First we look at predicting listeners\u2019 preference in spoken user generated content, namely, podcasts. Then, we predict popularity of news articles from several news agents in terms of the volume of comments they receive. We develop models for predicting the popularity of an article for both before and after it is published. Finally, we look at a different aspect of news impact: how reading a news article affects future user browsing behavior. In each setting, we find patterns that characterize the underlying behavior and extract features that we then use to establish models for predicting online behavior. I will defend my Ph.D. thesis on Wednesday, 5 December 2012, at 14:00 (GMT+1), at Agnietenkapel, Amsterdam. You are most welcome to join. In the meantime, please feel free to grab a copy , and cite the book: @phdthesis { tsagkias2012-thesis , Title = {Mining Social Media: Tracking Content and Predicting Behavior} , Author = {Manos Tsagkias} , Year = {2012} , School = {University of Amsterdam} }","title":"Ph.D. Thesis \u2014 Mining Social Media: Tracking Content and Predicting Behavior"},{"location":"posts/20130408-sigir2013-pseudo-test-collections/","text":"Abstract \u00b6 Recent years have witnessed a persistent interest in generating pseudo test collections, both for training and evaluation purposes. We describe a method for generating queries and relevance judgments for microblog search in an unsupervised way. Our starting point is this intuition: tweets with a hashtag are relevant to the topic covered by the hashtag and hence to a suitable query derived from the hashtag. Our baseline method selects all commonly used hashtags, and all associated tweets as relevance judgments; we then generate a query from these tweets. Next, we generate a timestamp for each query, allowing us to use temporal information in the training process. We then enrich the generation process with knowledge derived from an editorial test collection for microblog search. We use our pseudo test collections in two ways. First, we tune parameters of a variety of well known retrieval methods on them. Correlations with parameter sweeps on an editorial test collection are high on average, with a large variance over retrieval algorithms. Second, we use the pseudo test collections as training sets in a learning to rank scenario. Performance close to the training error on the editorial collection is achieved in all cases. Our results demonstrate the utility of tuning and training microblog search retrieval algorithms on automatically generated training material. Our paper Pseudo Test Collections for Training and Tuning Microblog Rankers by Richard Berendsen, Manos Tsagkias, Maarten de Rijke, and Wouter Weerkamp [ 1 ] has been accepted at SIGIR 2013, in Dublin, Ireland, 28 July\u20131 August. References \u00b6 [1] Richard Berendsen, Manos Tsagkias, Wouter Weerkamp, and Maarten de Rijke. 2013. Pseudo test collections for training and tuning microblog rankers . In Proceedings of the 36 th international ACM SIGIR conference on Research and development in information retrieval (SIGIR \u201813). Association for Computing Machinery, New York, NY, USA, 53\u201362. ACM Link PDF","title":"Paper at SIGIR 2013\u00a0\u2014\u00a0Pseudo Test Collections for Training and Tuning Microblog Rankers"},{"location":"posts/20140914-erd2014-semanticizing-search/","text":"Abstract \u00b6 This paper describes the University of Amsterdam\u2019s participation in the short track of the Entity Recognition & Disambiguation Challenge 2014 (ERD 2014). We describe how we adapt the Semanticizer \u2014an open-source entity linking framework developed primarily at the University of Amsterdam\u2014to the task of the ERD challenge: linking named entities in search engine queries. We steer the Semanticizer\u2019s linking towards named entities by adapting an existing training corpus, and extend the Semanticizer\u2019s set of features with contextual features that aim to leverage the limited context provided by search queries. With an F1 score of 0.6062 our final system run achieves median performance, and better than mean performance (0.5329). References \u00b6 [1] David Graus, Daan Odijk, Manos Tsagkias, Wouter Weerkamp, and Maarten de Rijke. 2014. Semanticizing search engine queries: the University of Amsterdam at the ERD 2014 challenge . In Proceedings of the first international workshop on Entity recognition & disambiguation (ERD \u201814). Association for Computing Machinery, New York, NY, USA, 69\u201374. ACM Link ; PDF .","title":"Paper at ERD 2014 \u2014\u00a0Semanticizing Search Engine Queries"},{"location":"posts/20160101-streamwatchr/","text":"Abstract \u00b6 Streamwatchr (2013\u20132016), developed at the University of Amsterdam, leveraged machine learning and named entity extraction to track global music listening behavior in real time through tweets. It offered features like Top-100 charts, interactive maps, and a \u201cradio\u201d stream mode driven by a dynamic recommender system. Using Python and MongoDB, it processed over 438 million tweets, identified 660,941 artists, and linked them to MusicBrainz and YouTube videos. This innovative blend of analytics and entity linking earned it a spot in the Dutch delegation at South by Southwest (SXSW). Streamwatchr (2013\u20132016) was a web application developed at the University of Amsterdam, by Wouter Weerkamp, Maarten de Rijke, and myself. Streamwatchr monitored the Twitter feed for #nowplaying, and tracked how people interacted with songs, artists, albums, in real-time. In its latest version it also identified which parts of the lyrics were the most sung along! Streamwatchr\u2019s innovative approach earned it a place in the Dutch delegation for innovation at the internationally renowned South by Southwest (SXSW) festival. Screencast of Streamtchr on YouTube. The video covers all the features of the platform. Streamwatchr provided unique insights into global music listening behavior through features such as Top-100 charts, interactive maps, and a \u201cradio\u201d stream mode. The radio stream played a sequence of YouTube videos, with each song following the next based on the probability of them being played together. This was achieved by linking identified songs to YouTube videos and generating playlists driven by a recommender system. Behind the scenes, the system utilized a directed graph, where nodes represented songs, and edge weights encoded the likelihood of transitioning from one song to another. These weights were dynamically updated with every incoming tweet. The engineering behind Streamwatchr was equally groundbreaking [ 1 ]. It collected music-related tweets in real time, extracted song and artist information, and mapped them to MusicBrainz, a comprehensive music database, and corresponding YouTube video clips. Every aspect of Streamwatchr, from popularity charts and trending music to song recommendations and analytics, was refreshed with each new tweet, ensuring a dynamic and up-to-date user experience. Streamwatchr leveraged a tech stack of Python and MongoDB for analytics and recommendations. Its real-time capabilities were powered by efficient data structures and algorithms that minimized the computational overhead required for updates, delivering a seamless and responsive experience. The millions of tweets and the hundreds of thousands of artists that Streamwatchr has listened to over the years have been distilled to a handful of noteworthy factoids: Factoid tweets listened 438,225,941 artists seen 660,941 most popular song Passenger\u2009\u2013\u2009Let Her Go, 196,986 times most sung along song John Legend\u2009\u2013\u2009All of Me, 541 times References \u00b6 [1]: Wouter Weerkamp, Manos Tsagkias, and Maarten de Rijke. Inside the world\u2019s playlist . In International Conference of Knowledge Management (ICKM) 2013. ACM Library ; PDF","title":"Streamwatchr \u2014 Inside the world's playlist"},{"location":"posts/20161210-dir/","text":"The Dutch-Belgian Information Retrieval Workshop was held this year in Delft. A nice lot of people from all around the Netherlands, Belgium, and a handful from abroad came together to share their research highlights from this year. It was a nice programme, filled with interesting short talks (about 15mins) that were great to give us a summary of what everybody is working on. My personal highlight was Chato\u2019s keynote speech on algorithmic bias. Although a lot is discussed on the topic, there is yet no to little work that presents some sort of foundation that describes the issue, frames it and offers some theoretical framework from which we can start thinking about solutions. Chato\u2019s work laid the ground works on this area. He started with explaining what algorithmic bias is and stressed how coupled it is with data collection. I\u2019m looking forward to seeing more on his work on the subject. Figure 1. \u201cDetecting Algorithmic Discrimination\u201d by Carlos Castillo, presented at DIR 2016. Delft, the Netherlands.","title":"Attending DIR 2016"},{"location":"posts/20161210-sigir/","text":"Figure 1. \u201cBuilding a Self-Learning Search Engine: From Research to Business\u201d presented at SIGIR 2016 Industry Track. Pisa, Italy. This summer I was happy to be back at the Special Interest Group in Information Retrieval (SIGIR), this time, in Pisa, Italy. Last time I was at SIGIR was in Portland back in 2012, where I was presenting the work I did at Yahoo! Barcelona. SIGIR is the top international conference on search engines, and it\u2019s always good fun to hear the latest developments and exchange new ideas. This year I talked at the industry track where I shared my experiences from running a search startup in the Netherlands. I received positive feedback, and it was good to catch up with old friends and meet new people. If you want a short recap from the latest and greatest trends in search technology, read on my summary on 904Labs blog .","title":"Presenting at the Industry Day at SIGIR 2016"},{"location":"posts/20170418-historical-moment-904labs/","text":"I\u2019m happy to share some great results with regards to the utility of self-learning search and revenue on e-commerce sites. 904Labs self-learning search improves revenue by 36% when compared to a highly, but manually, tuned Apache Solr search engine. Good job 904Labs team! Read the full blog post at 904Labs, here: https://www.904labs.com/en/self-learning-search-improves-revenue-for-e-commerce","title":"A historical moment for 904Labs"},{"location":"posts/20171021-ai2future/","text":"Figure 1. At the stage of AI2Future. AI2Future is a local, Croatian, initiative for disseminating the importance and for grounding the use of Artificial Intelligence. The conference brings together researchers, A.I. start ups, and large corporates from all around Croatia to share experiences and learn from each other on what A.I. can and cannot do. I was invited to give one of the two keynote talks. The first keynote focused on conversational agents and natural language processing and mine followed up with insights how search powers a large spectrum of applications from search, question and answering, and recommendations. I focused on the work we do at 904Labs and illustrated the principles of online learning to rank through real-world examples from our experimences with our customers. I got many questions after the talk which is a sign that the audience understood the topic and was intrigued by what A.I. can do for search. Some of the questions were brought to the following breaks with some of them leading to follow up meetings in the next days. I much enjoyed the conference, and I hope that the organizers will follow up with another version next year. I believe that we do need more of this type of initiative to disseminate what A.I. is and A.I. can and cannot do so that organizations shape a better picture of how they can use it to solve challenges that they face. More pictures from the event.","title":"Keynote speech at AI2Future"},{"location":"posts/20171104-another-historical-moment-904labs/","text":"I\u2019m very happy to share news on the latest success of 904Labs A.I. for Search with our new customer, eci.nl . eci is one of the largest online book specialists in the Netherlands and Belgium, with an estimated yearly revenue of 15m euro for the Dutch part. For their site search, eci relied on a manually optimized Apache Solr, integrated into Intershop. During a three-week A/B test we\u2019ve shown a 38% improvement in revenue and a 34% increase in conversion rate compared to the in-house Apache Solr search engine. This shows once more that adding A.I. to your search engine really makes a difference! Read the full story here.","title":"Another historical moment for 904Labs"},{"location":"posts/20171213-ams-city/","text":"I was happy to be invited at the Amsterdam City A.I. Event on December 11, 2017 . My talk revolved around our experiences at 904Labs in building an A.I. focused company. It was fun to be among A.I. enthusiasts and to see that people identified with our experiences\u2013which means that we are on good track! The talk is online on YouTube (clicking the link will start playing the video at the start of my talk): Recording of the talk on YouTube. Below are some tweets from the event: https://twitter.com/amsterdam_ai/status/940286324178915339 https://twitter.com/amsterdam_ai/status/940286970646888449 https://twitter.com/amsterdam_ai/status/940291725049974786","title":"Talk at Amsterdam City A.I. Event"},{"location":"posts/20180211-emakina/","text":"Emakina, one of the largest web agencies in Europe, works with internationally acclaimed brands on their branding and electronic presence. To keep their customers ahead of the curve, Emakina has recently started a series of meetups where experts in a wide range of fields come and talk about the latest developments in their field. The last meetup was held last Thursday, 8 February 2018, and with the topic: \u201cA.I. for Commerce\u201d, three talks were scheduled: one from Emakina, one from 904Labs, and one from Salesforce. Tweet from the event. In our talk, I described the importance of search in e-commerce by giving examples of failed searches in a number of settings, from finding advertized items using onsite search to mobile search. I followed by with why people choose Amazon to start their product search and highlighted that 54% of them choose Amazon because of their great search functionality\u2013that is the reason number five for people to go to Amazon. Then, I explained why optimizing the ranking manually is close to impossible for humans by laying out the insane amount of options available and enumerating the search space (which can be at millions of millions of choices). With this as foundation, I talked about machine learning and the particular type of machine learning that we use at 904Labs for optimizing search rankings in real-time. I followed up with describing our query intent engine, which is powered by 904Sense, and our automatic synonym extraction engine. In my conclusions, I re-iterated that A.I. for Search can boost search-driven revenue by 30% and that search is becoming part of platforms. In this angle, it is important for online retailers to test the claims of their vendors by doing A/B tests before they opt in for a solution.","title":"Talk at A.I. for Commerce meetup"},{"location":"posts/20180308-frankfurt/","text":"Recording of the talk is on YouTube; courtesy of Frankfurt Data Science. I was happy to be invited at Frankfurt Data Science meetup to talk about data science, meetups, and how to build a data science-oriented startup. The event was held at Frankfurt School on March 1 st , 2018 . In my talk I gave an overview of the meetup scene in Amsterdam, briefly presented Amsterdam Data Science and its activites, and then I shared my experiences on founding and developing 904Labs , before I delved into one of my favorite topics: machine learning and search. It was a packed room, with more than 200 registered people for the event, and the talk was broadcasted live on YouTube. There will soon be a video, which I will share here. The audience was from diverse backgrounds, and I enjoyed the interactions very much. I was happily surprised by the professional organization of the event, and the ambition for making Frankfurt one of the leading centres in data science in Europe. All the best to the organizers and I hope to see more collaboration on data science between Frankfurt and Amsterdam in the near future!","title":"Talk at Frankfurt Data Science meetup"},{"location":"posts/20180330-ecir2018/","text":"Starting my talk at ECIR 2018 Industry track. Courtesy of Gabriela Kazai. European Conference on Information Retrieval (ECIR) is a annual European scientific conference around bhe topics of search engines, recommender systems, text analytics, user modeling, and evaluation. This year ECIR was held in Grenoble, France. With more than 250 attendants and 4 days packed with tutorials, workshops, research, and industry talks, it was a great place to be to get updated with the latest and greatest about search engines. The theme of this year\u2019s Industry Day was to bring lessons learned from industry to academia. What are differences when developing a research algorithm and when we bring to practice? These lessons could inspire and inform our fellow academics for the challenges practitioners face when bring these algorithms to production. In my talk I touched upon a bunch of challenges we\u2019ve faced at 904Labs and how we came about to solving them. Open questions revolve around online learning to rank algorithms, delayed feedback, design of new metrics to avoid embarassing results, and the importance of investing in an evaluation platform. At 904Labs we have come a long way and we have working answers for these questions, however, as these questions are particularly difficult to answer, I\u2019ve invited people to drop me a note with ideas, if they are interested. I\u2019ve already got some nice feedback, and I hope to see more research on these areas in the near future! Below are some tweets from my talk. https://twitter.com/gkazai/status/979334863244447745 https://twitter.com/gkazai/status/979336944944648192 https://twitter.com/miguelmalvarez/status/979338390322720769","title":"Talk at ECIR 2018 Industry day"},{"location":"posts/20180601-techloop/","text":"View from the stage at Techloop meetup. On 31 May, I was invited to give a talk at TECH Talks in Amsterdam . TECH Talks is a new meetup organized by Techloop.io , a new way for matching jobs and talent in IT. My talk revolved on how we built an e-commerce focused search engine using machine learning (or as most people know it, A.I.). The meetup had a great start with more than 200 registered people and more than 100 people showed up; the room at TQ was at its maximum capacity! The audience was diverse with a nice mix of frontend, backend , senior and junior engineers, and also people from other disciplines who are interested in keeping up with latest developments in IT and e-commerce. Techloop and TQ were great organizers providing a great atmosphere (pizza and beer, included) facilitating great chats and networks afterwards. You can see the video of the talks here (mine is the first after the introduction by Techloop):","title":"Talk at TECH Talks Amsterdam"},{"location":"posts/20180618-query-engine-upgrade/","text":"Content Some relatively simple cases Some more difficult cases Some very difficult cases Today at 904Labs Search we\u2019ve been having fun with the next version of our Query Intent/Understanding engine. If you don\u2019t know what a Query Intent/Understanding engine is, here\u2019s some background ; otherwise keep reading. Our Query Intent/Understanding engine is closing to one year old, and it\u2019s pretty cool: it can quickly boostrap a knowledge graph and continuously update it straight from our customer\u2019s data. Since its introduction in September 2017, it has been tested in many different domains and languages and has shown substantial revenue uplift across customers. In the last couple of weeks, our team has been busy tweaking our engine and pushing it even further. Our Query Intent/Understanding engine was doing already a good job, mapping query free text to product attributes such as category, or brands. However, we\u2019ve never tested going deeper into particular product attributes, such the product weight or even, wattage. Our engineers managed to extend the logic and the scalability of the algorithm to support very fine grained suggestions, if such fine-grained information exists in the index. It is quite a feat and I\u2019m very proud of our team! On top of the engineering feat, the algorithm itself is pretty lean and has quite a few distinctive features that are hard to find in other engines: it does not depend on external dependencies (read Wikipedia), nor it needs periodic re-training. The only thing that a customer has to do is to plug in our system to a Apache Solr/Elasticsearch, and 904Labs Query Intent engine is built incrementally, one click/add-to-basket/purchase at a time. Oh, and it doesn\u2019t write anything on your index. Magic! Here\u2019s a few interesting examples that came out from today\u2019s experimentation: (the examples are in Dutch; I try to translate the queries into English. Field attributes and values may be partially redacted for anonymity) Some relatively simple cases \u00b6 The system learns to map the query: lampen (lamps) to Solr\u2019s directive: category:Lampen . Learning of this mapping is automatic and based on only from user interactions. Query: Tent \u2192 category:Tenten . Note how the system has learned the mapping from singular to plural without using of manual synonyms. Query: Stoel kussens (chair pillows) \u2192 category:Kussens (pillows). In this example, the system maps a specific query to a broader category. This highlights a productive synergy between human and machine: the site\u2019s taxonomy lacked a dedicated leaf node for chair pillows, so the system intelligently selected the closest broader category. This choice influences the downstream learning-to-rank process, bringing greater focus to the broader category of pillows. Some more difficult cases \u00b6 Query: Bed 160x200 \u2192 size:160x200 cm and size:160 x 200 cm . Note how the system has learned the discrepency in attribute values and suggests both; in the first one there are no spaces in the representation of the dimension, in the second one there are. Query: stoelen (chairs) \u2192 category:... en eetkamerstoelen (kitchens and dining chairs). Here the system attempts to infer the intented category for a broad query such as \u201cchairs\u201d. Although there is a category labeled \u201cchairs\u201d, the system decides to suggest a particular leaf within the taxonomy branch of chairs. Query: tuinset (garden set) \u2192 number_of_persons:12 . The system implicitly defines the size and dimensions of the garden set from historical user behavior. Given the lack of a user profile, suggestions like this aim at capturing the interests of the average user of the shop. Some very difficult cases \u00b6 Finally, two of my favorites, where the system attempts to infer several attributes of products from broad queries\u2014 Query: prieel (garden house) \u2192 material:XXX, color:Beige, weight:4.9 kg . Here the system maps a broad query to distinct set of attributes, i.e., the material, the color, and even the weight of the garden house. Again, with no user profile provided, these suggestions attempt to capture the interests of the average user of the shop. Query: eenpersoons bed (one person bed) \u2192 size:90 x 200 cm . The system maps the query to exact dimensions for the bed! Query: XXX lounge (brand redacted) \u2192 model:White XXX . Here the system manages to map a brand and type of products from this brand to a specific product model from this brand. Again, without user profiling. Quite impressive! I hope you also find exciting what the new version of 904Labs Query Intent/Understanding engine can do. You can have this technology working for you too and straight away, by getting in touch with us . Plus, unlike other search engine providers, with us you keep your index, and we don\u2019t touch it so we don\u2019t lock you in. The first month is for free, terminable monthly.","title":"New Version for 904Labs Query Intent Engine"},{"location":"posts/20180618-query-engine-upgrade/#some-relatively-simple-cases","text":"The system learns to map the query: lampen (lamps) to Solr\u2019s directive: category:Lampen . Learning of this mapping is automatic and based on only from user interactions. Query: Tent \u2192 category:Tenten . Note how the system has learned the mapping from singular to plural without using of manual synonyms. Query: Stoel kussens (chair pillows) \u2192 category:Kussens (pillows). In this example, the system maps a specific query to a broader category. This highlights a productive synergy between human and machine: the site\u2019s taxonomy lacked a dedicated leaf node for chair pillows, so the system intelligently selected the closest broader category. This choice influences the downstream learning-to-rank process, bringing greater focus to the broader category of pillows.","title":"Some relatively simple cases"},{"location":"posts/20180618-query-engine-upgrade/#some-more-difficult-cases","text":"Query: Bed 160x200 \u2192 size:160x200 cm and size:160 x 200 cm . Note how the system has learned the discrepency in attribute values and suggests both; in the first one there are no spaces in the representation of the dimension, in the second one there are. Query: stoelen (chairs) \u2192 category:... en eetkamerstoelen (kitchens and dining chairs). Here the system attempts to infer the intented category for a broad query such as \u201cchairs\u201d. Although there is a category labeled \u201cchairs\u201d, the system decides to suggest a particular leaf within the taxonomy branch of chairs. Query: tuinset (garden set) \u2192 number_of_persons:12 . The system implicitly defines the size and dimensions of the garden set from historical user behavior. Given the lack of a user profile, suggestions like this aim at capturing the interests of the average user of the shop.","title":"Some more difficult cases"},{"location":"posts/20180618-query-engine-upgrade/#some-very-difficult-cases","text":"Finally, two of my favorites, where the system attempts to infer several attributes of products from broad queries\u2014 Query: prieel (garden house) \u2192 material:XXX, color:Beige, weight:4.9 kg . Here the system maps a broad query to distinct set of attributes, i.e., the material, the color, and even the weight of the garden house. Again, with no user profile provided, these suggestions attempt to capture the interests of the average user of the shop. Query: eenpersoons bed (one person bed) \u2192 size:90 x 200 cm . The system maps the query to exact dimensions for the bed! Query: XXX lounge (brand redacted) \u2192 model:White XXX . Here the system manages to map a brand and type of products from this brand to a specific product model from this brand. Again, without user profiling. Quite impressive! I hope you also find exciting what the new version of 904Labs Query Intent/Understanding engine can do. You can have this technology working for you too and straight away, by getting in touch with us . Plus, unlike other search engine providers, with us you keep your index, and we don\u2019t touch it so we don\u2019t lock you in. The first month is for free, terminable monthly.","title":"Some very difficult cases"},{"location":"posts/20180618-query-engine/","text":"In the past year, at 904Labs A.I. for Search , we\u2019ve put a lot of effort to optimize and push further the technology behind query intent/understanding engines. If you are in the search engine/information retrieval community, you know the term and how challenging the problem is. But if you are not familiar with the problem, it is hard to grasp its importance and challenges. Here\u2019s my attempt to explain it in simple words. A Query Intent/Understanding engine aims at identifying the intent of a user\u2019s query and ultimately at translating it to a set of search directives. Consider a user coming to your e-shop and typing in the search box: \u201cred shoes\u201d. The query may look pretty straightforward to you when it comes to what products to show to the user but, for a machine, it\u2019s pretty hard to figure out what the person meant. To get into the machine\u2019s shoes (pun intended) think for a moment that you were born and raised in a warehouse, which is isolated from the rest of the world but has all the inventory of an e-shop. You\u2019ve never left the warehouse so you don\u2019t know anything about the world. It\u2019s quite grim world but bare with me. One day someone slips a message under the door of the warehouse with the words \u201cred shoes\u201d. Now your task is to select a set of products that satisfy/cover that person\u2019s information need. Obviously you know nothing about that mysterious person, let alone their preferences, and you barely understand the language of the message\u2013perhaps you are able to recognize characters and words and match them to words found on the labels of products, but that\u2019s pretty much it. In such a surreal world, you can imagine that it is quite difficult, even for you, a human, to select a set of products that relate to \u201cred shoes\u201d. The challenge lies in that much of the important, contextual, information that we have access to from our constant interactions with the real world by living in it, it is very much missing in this artificial setting: We don\u2019t know whether the user is a he or a she (and therefore we don\u2019t know if we should pick male or female type of shoes), we don\u2019t know what is the \u201chot\u201d color of the season, nor the most popular brand, nor whether the person is looking for sneakers or boots or high-heels. There are lots of unknowns. A Query Intent/Understanding engine is an algorithm that tries to make sense of the world for machines, or other entities, which are locked up in a similar warehouse as the one we described above and have no access to contextual information. A Query Intent/Understanding algorithm aims at mapping free text (a user query) to a series of directives (rules) that when applied, they will yield a useful set of products for the user. In our \u201cred shoes\u201d example, we are looking for directives that look as the following: \u201cfilter on products that have attribute:red and category:shoes\u201d. At a first glance, the mapping looks simple but as of now you\u2019ve seen from our warehouse setting that it can be quite daunting. Researchers and practitioners have been working on this problem for quite some time and progress has been made; however the community has still some way to go before solving it. At the core of current solutions, there is a lot of complex technology such as neural networks that power NLP (Natural Language Processing) tools and other pipelines that require lots of human annotations (read expensive). These approaches yield relatively good accuracy, however, putting these systems into production may still be a challenging engineering problem\u2013from setting up the data pipelining to scaling up, to retraining, and to monitoring system effectiveness and efficiency. At 904Labs we\u2019ve developed a Query Intent/Understanding engine that hooks up on an e-shop\u2019s data, bootstraps a knowledge graph, and it learns the mapping to directives automatically, i.e., without supervision (read without human annotations). It doesn\u2019t require retraining nor external dependencies, and you can use it straight away, today, on your own Apache Solr or Elasticsearch index (from which we only read and never write and we never lock you in). If that sounds appealing, get in touch for a one month free trial!","title":"What is a Query Intent/Understanding engine?"},{"location":"posts/20180702-aiexpo/","text":"904Labs exhibitor's booth at A.I. Expo Europe at RAI, Amsterdam, the Netherlands. This year we were invited to be part of A.I. Expo Europe, held in Amsterdam. The organizers were very kind to offer us a free booth in the Startup zone of the exhibition. The event took place on 27 and 28 June 2018, and it was packed with talks and visitors. There were three tracks: A.I., IoT, and Blockchain, with the latter two dominating the exhibitors list. It was an intense two days, with lots of people dropping by 904Labs\u2019 booth and asking about what we do and how 904Labs search technology can help them in their settings. Altough the average visitor profile was not looking for onsite search, we had good chats with a bunch of Dutch and international and small, medium, and big companies on how onsite search can help their endeavors. We talked on how search technology is underneath many applications, including chatbots, and self-service applications. No matter the application, good search can help increase revenue in the setting of selling products, or reduce costs in the setting of self-service as visitors are better able to find the information they\u2019re looking for without calling the company\u2019s call centre. Besides core business, it was nice to see other startups and non-startups from Amsterdam and catch up with old faces and meet new ones. Overall, it was a fun experience. A big thanks to the A.I. Expo Europe organizers for inviting us over!","title":"Exhibiting at A.I. Expo Europe 2018"},{"location":"posts/20190718-ecommerce-workshop/","text":"I was invited to give a talk at the SIGIR Workshop On eCommerce 2019 but unfortunately, I am not attending SIGIR this year. Instead I wrote down some thoughts on interesting problems, ideas and challenges in the e-commerce domain. Here they are: The vocabulary gap is still an open problem. \u00b6 People refer to products in different ways that products are described in the catalogue. Neural networks, learning to rank, query intent engines are all important. Our experience at 904Labs shown that a query intent engine that boosts specific product categories given a query, boosts our learning to rank system by more than 16% in additional revenue. These results suggest that effective initial ranking is very important for effective learning to rank. To this end, we foresee an increasing interest in methods that can re-rank the entire collection and not only the top-N documents. This is particularly important for larger shops with large inventories (more than hundred of thousands of items) where a query can return hundreds of items, and only the top few are re-ranked. E-commerce search is as much as about exploration as it is about finding the best match. \u00b6 From our experience at 904Labs, we see a large fraction of queries to revolve around categories, or combinations of categories, e.g., red shoes , kitchen tables , dvd players , ebooks for 12 years old . This type of queries go beyond our typical search and require understanding the query and generating a list of recommended relevant items. This proposition is supported by the surprising effectiveness of sorting by popularity; the most popular items for a query are potential good candidates for this \u201crecommendation list\u201d that the user is looking for. Back to query understanding of this type of exploratory queries, one would think that natural language processing can help here but in the e-commerce setting, queries are very short and any language analysis falls short. An open question here is how can we go from these broad queries to a good set of recommendations? A natural way is to devise a hybrid system of search and recommendations: We fire the query to a search system and then we take the first few items as seed to a recommender system for getting similar items. Or a system that transforms a query to an image (think AttnGAN or similar) and each product to an image and then rank documents by their image similarity to the query\u2019s\u2013the image representation may constraint the latent space, abstract the language of the query and that of the document and be able to capture semantics that are otherwise difficult to encode in textual form. The image representation of queries and documents also offers explainability when it comes to explaining the rankings of the system; which is becoming increasingly important in machine learning-based systems. Evaluation metrics in e-commerce. \u00b6 There are several directions here that we need do more work. First, the e-commerce setting is a conjunction of exploratory search, typical search, and recommendations. Using the standard IR precision and recall measures for e-commerce may not tell us the entire story for how happy makes its users. We need to discover the aspects of a system that makes users happy for designing one or more metrics for evaluating search and recommendation systems. These metrics should also correlate well with revenue but also with customer loyalty (measured in returning customers and in shortening the time between returns). Such a metric (or a multiple of metrics) will then allow us to run offline experiments and make predictions on revenue, which is the main KPI that systems are evaluated in production for most e-commerce business. Last, an extra tip: we found that boolean scoring may be on par or outperform tf.idf or BM25 scoring in the e-commerce domain, it\u2019s worth checking its effectiveness on your own data ;)","title":"Inlieu of an Invited Talk\u2014Thoughts on the Future of Search in E-commerce"},{"location":"posts/20190718-ecommerce-workshop/#the-vocabulary-gap-is-still-an-open-problem","text":"People refer to products in different ways that products are described in the catalogue. Neural networks, learning to rank, query intent engines are all important. Our experience at 904Labs shown that a query intent engine that boosts specific product categories given a query, boosts our learning to rank system by more than 16% in additional revenue. These results suggest that effective initial ranking is very important for effective learning to rank. To this end, we foresee an increasing interest in methods that can re-rank the entire collection and not only the top-N documents. This is particularly important for larger shops with large inventories (more than hundred of thousands of items) where a query can return hundreds of items, and only the top few are re-ranked.","title":"The vocabulary gap is still an open problem."},{"location":"posts/20190718-ecommerce-workshop/#e-commerce-search-is-as-much-as-about-exploration-as-it-is-about-finding-the-best-match","text":"From our experience at 904Labs, we see a large fraction of queries to revolve around categories, or combinations of categories, e.g., red shoes , kitchen tables , dvd players , ebooks for 12 years old . This type of queries go beyond our typical search and require understanding the query and generating a list of recommended relevant items. This proposition is supported by the surprising effectiveness of sorting by popularity; the most popular items for a query are potential good candidates for this \u201crecommendation list\u201d that the user is looking for. Back to query understanding of this type of exploratory queries, one would think that natural language processing can help here but in the e-commerce setting, queries are very short and any language analysis falls short. An open question here is how can we go from these broad queries to a good set of recommendations? A natural way is to devise a hybrid system of search and recommendations: We fire the query to a search system and then we take the first few items as seed to a recommender system for getting similar items. Or a system that transforms a query to an image (think AttnGAN or similar) and each product to an image and then rank documents by their image similarity to the query\u2019s\u2013the image representation may constraint the latent space, abstract the language of the query and that of the document and be able to capture semantics that are otherwise difficult to encode in textual form. The image representation of queries and documents also offers explainability when it comes to explaining the rankings of the system; which is becoming increasingly important in machine learning-based systems.","title":"E-commerce search is as much as about exploration as it is about finding the best match."},{"location":"posts/20190718-ecommerce-workshop/#evaluation-metrics-in-e-commerce","text":"There are several directions here that we need do more work. First, the e-commerce setting is a conjunction of exploratory search, typical search, and recommendations. Using the standard IR precision and recall measures for e-commerce may not tell us the entire story for how happy makes its users. We need to discover the aspects of a system that makes users happy for designing one or more metrics for evaluating search and recommendation systems. These metrics should also correlate well with revenue but also with customer loyalty (measured in returning customers and in shortening the time between returns). Such a metric (or a multiple of metrics) will then allow us to run offline experiments and make predictions on revenue, which is the main KPI that systems are evaluated in production for most e-commerce business. Last, an extra tip: we found that boolean scoring may be on par or outperform tf.idf or BM25 scoring in the e-commerce domain, it\u2019s worth checking its effectiveness on your own data ;)","title":"Evaluation metrics in e-commerce."},{"location":"posts/20191117-goodbye-904labs/","text":"The end of 2019 will mark the fifth anniversary of 904Labs. Wouter and I founded 904Labs in late 2014 and embarked in an adventure of becoming entrepreneurs while holding tight to our scientist trait. We took courses in entrepreneurship, joined our university\u2019s startup incubator, poured all the money we had saved by that moment in the company, and we set sail for bringing state-of-the-art search to the masses. A lot has happened in these five years. We met with incredible people in all ranks, involved in from local startups to worldwide multinationals. Each one helped us in their own way, from validating our ideas, shaping our product-A.I. for Search-, to running pilots. We received support from our university, University of Amsterdam, through grants and loans, which helped us go through the development phase of our product and bring it to the market. Within a year since we started, we grew from just two founders to a team of six, and our code base went from a bunch of scripts to a full blown, distributed, horizontally scalable, multi-tenant architecture. In our second year we ran our first pilot, with massive success: A.I. for Search managed to increase search driven revenue by 38% ! The same year we had our first big customer and the year after a second, bigger one. By 2019, A.I. for Search has served searches from more than 15 countries and an equal number of languages. It has battled and won in more than 10 A/B tests, in all of which it has proven to substantially increase search driven revenue. Tech-wise, 904Labs has been success story, really. Business-wise, the story is slightly different. Wouter and I, our business developers and salespeople, found it hard to sell A.I. for Search. Customers were excited with the tech and the potential, but the deals were not closing. We\u2019ve been banging our heads against the wall for a long time but didn\u2019t manage to find a solution to why the deals were not closing, at least not early enough. Three years in and without a good customer base, we\u2019ve hit a financial storm. We had to let all of our people go, and our offices as well. Despite the financial stress, we decided to keep the company running while hoping for better days; we minimized all possible expenses to squeeze out all the time we could buy until new deals come in\u2013which would allow us to reboot the company. Two years since, we didn\u2019t manage to close new deals, but the epiphany we were looking for since our early days stroke us: deals weren\u2019t closing because we\u2019ve been targeting a very narrow and difficult customer segment, which was insufficient to sustain the company growth we needed and wished for. The realization hit us hard. We spent a lot of time thinking of alternatives but couldn\u2019t find a satisfying alternative. In the beginning of 2019, one of our two customers left, and their departure marked the beginning of winding down 904Labs. We decided that despite the difficult customer segment we chose to target, 904Labs\u2019 tech was still magical and had to remain with the world no matter what was the future of 904Labs as company. We\u2019ve reached out to anyone and any entity that we believed our tech will be useful: our professional network, potential customers, competitors. We offered a one time license to our source code. After taking hold of the code one would be able to do whatever they want with it. In the one time license fee, we included our latest version of our code base, our infrastructure, documentation, our processes, and, of course, onsite training and support. We\u2019ve been actively talking with a few companies now, and they will bring in A.I. for Search to either complement and strengthen their offering, or to use it internally in different projects. The next few quarters may bring the company of 904Labs to an end, but its tech will survive, and it will keep helping people find what they\u2019re looking for; by learning from one click at a time, as it used to. As to what Wouter and I are up to next, it has been a tough decision but we\u2019re on good and exciting new paths. Wouter has joined Zeta-Alpha-Vector , a Dutch deep learning research company based in Amsterdam, and I\u2019m joining Apple in California to work with the Siri Understanding team. While at 904Labs, we both learned so much: building and running a business, developing a product, managing engineering and sales teams. It\u2019s been an invaluable experience. 904Labs gifted us the knowledge of what it takes to build a product in the real-world, which is very different from what we had in mind when we left Academia. By now we know that a product needs to be fun from a tech/scientific perspective but it is equally important to satisfy customers needs and solve specific problems\u2013at the same time it is bound by limited resources, either be it time, people, or budget. Finding the right balance on all three dimensions is key to product success. The lesson may have come late for 904Labs, but we are excited to be able to apply this hard earned knowledge as well as earlier and newly acquired skills to new environments and challenges! Before waving goodbye, we would like to take a moment and extend a big thank you to everyone who has been involved with 904Labs, directly or indirectly: Our customers, engineers, sales people, university, mentors, shareholders, network, and last, but not least, our families and friends who\u2019ve been very supportive throughout this entire journey. It\u2019s been a fun ride but not always easy. We wouldn\u2019t have made it so far without their support. Thank you!","title":"904Labs: Goodbye and thanks for all the clicks!"},{"location":"posts/20191118-joining-apple/","text":"Today marks my first day at Apple! I\u2019ve joined the Apple Speech Recognition team within Siri, part of the AI/ML (Artificial Intelligence and Machine Learning) organization. After 12 wonderful years in Amsterdam, I\u2019ve relocated to Cupertino, California, in the United States. I\u2019m thrilled to be working at Apple Park, one of the most iconic and beautiful buildings in the world. Excited for the journey ahead!","title":"Joining Siri at Apple"},{"location":"posts/20200622-sigir-forum-ecommerce/","text":"Abstract \u00b6 With the rapid adoption of online shopping, academic research in the eCommerce domain has gained traction. However, significant research challenges remain, spanning from classic eCommerce search problems such as matching textual queries to multi-modal documents and ranking optimization for two-sided marketplaces to human-computer interaction and recom- mender systems for discovery and browsing. These research areas are important for under- standing customer behavior, driving engagement, and improving product discoverability and conversion. In this article we identify the challenges and highlight research opportunities to improve the eCommerce customer experience. As eCommerce is becoming increasingly important, we recruited a small team of researchers from major and not so major players in the field to share views and experiences on theoretical and practical challenges and future directions on eCommerce search and recommendations. The result of our effort, Challenges and Research Opportunities in eCommerce Search and Recommendations is published in June 2020 issue of SIGIR Forum . This work is related to thoughts that I shared earlier on product search , and to 904Labs\u2019 Query Intent engine and its magnificent results (examples) .","title":"Paper at SIGIR Forum \u2014 Challenges and Research Opportunities in eCommerce Search and Recommendations"},{"location":"posts/20200722-sigir2020/","text":"Abstract \u00b6 We focus on improving the effectiveness of a Virtual Assistant (VA) in recognizing emerging entities in spoken queries. We introduce a method that uses historical user interactions to forecast which entities will gain in popularity and become trending, and it subse- quently integrates the predictions within the Automated Speech Recognition (ASR) component of the VA. Experiments show that our proposed approach results in a 20% relative reduction in errors on emerging entity name utterances without degrading the overall recognition quality of the system. Happy to share the news about my first joint pubication with the Siri Speech team at Apple. Our short paper Predicting Entity Popularity to Improve Spoken Entity Recognition by Virtual Assistants with Christophe van Gysel, myself, Ernie Pusateri, and Ilya Oparin, is accepted at SIGIR 2020.","title":"Paper at SIGIR 2020 \u2014 Predicting Entity Popularity to Improve Spoken Entity Recognition by Virtual Assistants"},{"location":"posts/20210214-icassp/","text":"Abstract \u00b6 Language models (LMs) for virtual assistants (VAs) are typically trained on large amounts of data, resulting in prohibitively large models which require excessive memory and/or cannot be used to serve user requests in real-time. Entropy pruning results in smaller models but with significant degradation of effectiveness in the tail of the user request distribution. We customize entropy pruning by allowing for a keep list of infrequent n-grams that require a more relaxed pruning threshold, and propose three methods to construct the keep list. Each method has its own advantages and disadvantages with respect to LM size, ASR accuracy and cost of constructing the keep list. Our best LM gives 8% average Word Error Rate (WER) reduction on a targeted test set, but is 3 times larger than the baseline. We also propose discriminative methods to reduce the size of the LM while retaining the majority of the WER gains achieved by the largest LM. Happy to share yet another publication with the Siri Speech team at Apple, this time led by Sashank Gondala, who interned with us last year. Our full paper Error-driven Pruning of Language Models for Virtual Assistants is accepted at ICASSP 2021.","title":"Paper at ICASSP 2021 \u2014\u00a0Error-driven Pruning of Language Models for Virtual Assistants"}]}