<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <!--
        for IE/Edge only
    -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <meta name="author" content="Manos Tsagkias">
    <link rel="canonical" href="https://manostsagkias.com/posts/20101002-kl-divergence/">
    <!--
        The page_title contains the title for a page as shown in the navigation.
        Site name contains the name as defined in the mkdocs.yml
    -->
    <title>KL-divergence of Two Documents - Connecting the Dots</title>
    <!--
        Just add a favicon.ico image to the docs.
    -->
    <link rel="shortcut icon" href="../../img/favicon.ico">
    <link rel="apple-touch-icon" sizes="180x180" href=../../img/apple-touch-icon.png>
    <link rel="icon" type="image/png" sizes="32x32" href=../../img/favicon-32x32.png>
    <link rel="icon" type="image/png" sizes="16x16" href=../../img/favicon-16x16.png>
    <link rel="manifest" href=/site.webmanifest>
      <link href="https://cdn.jsdelivr.net/npm/markdown-it-texmath/css/texmath.min.css" rel="stylesheet">
      <link href="https://cdn.jsdelivr.net/gh/goessner/mdmath/themes/publication/style.css" rel="stylesheet">
      <link href="../../css/custom.css" rel="stylesheet">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Alex+Brush&family=Allura&family=Dancing+Script:wght@400..700&family=Great+Vibes&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
    <link rel="stylesheet" href="../../css/styles.css">
    <script>
        // Inject the current file name for the active page
        const CURRENT_FILE = "posts/20101002-kl-divergence/";
    </script>
</head>
<body><div id="map-overlay" class="hidden">
    
    <button id="map-close" onclick="toggleMap()">&times;</button>
    <main>
<article>
<p>
    Hover over the dots to explore related posts. Closer dots are more semantically related, and the red dot marks the current page. 
</p>
</article>
</main>
    
    <div id="map-container"></div>
    <div id="search-container">
        <input id="search-box" type="text" placeholder="Type to search the map ..." />
    </div>    
</div>

    
    <header>
        <!-- Navigation menu -->
        
        <nav>
            <ul class="nav-bar">
                <!-- Left-aligned 'Home' link -->
                <li class="nav-left">
                    <a href="../../">Home</a>
                </li>
                
                <li class="nav-right">
                    <!-- Dynamic Navigation Items from `nav` -->
                    
                        <a href="../../about/">About</a>
                    
                    <!-- Right-aligned 'Map' link -->
                    <a href="#" onclick="toggleMap()">Map</a>
                </li>
            </ul>
        </nav>
        

        
            <h1>KL-divergence of Two Documents</h1>

<h3>A hands-on tutorial</h3>


    
        <h4>Manos Tsagkias</h4>
    


<h5>University of Amsterdam<br/><a href='mailto:hi@manostsagkias.com?subject=Hi%20from%20manostsagkias.com!'>hi@manostsagkias.com</a></h5>


<h5>06 December 2010</h5>


<h5><b>Keywords:</b> information theory</h5>

        
    </header>

    <main>
        <article>
    <blockquote>
<p><em>6 December 2024—Note:</em> This post is a restoration of an older post, which is no longer available online. Content-wise this version is identical to the original. The original version can still be found in the <a href="http://web.archive.org/web/20130508191111/http://staff.science.uva.nl/~tsagias/?p=185">WayBack Machine</a>.</p>
</blockquote>
<div class="toc"><span class="toctitle">Content</span><ul>
<li><a href="#working-with-documents">Working with documents</a></li>
<li><a href="#symmetric-kl-divergence">Symmetric KL-divergence</a></li>
<li><a href="#over-which-random-variables">Over which random variables?</a></li>
<li><a href="#simple-back-off">Simple back-off</a></li>
<li><a href="#the-code">The code</a></li>
</ul>
</div>
<p>Let <span class="arithmatex">\(P\)</span> and <span class="arithmatex">\(Q\)</span> be two probability distributions of a discrete random variable. If the following two properties hold:</p>
<ol>
<li>when <span class="arithmatex">\(P\)</span> and <span class="arithmatex">\(Q\)</span> both sum to <span class="arithmatex">\(1\)</span></li>
<li>and for any <span class="arithmatex">\(i\)</span> such that <span class="arithmatex">\(P(i) &gt; 0\)</span> and <span class="arithmatex">\(Q(i) &gt; 0\)</span></li>
</ol>
<p>then, we can define their KL-divergence as:</p>
<div class="arithmatex">\[D_{KL}(P||Q) = \sum_{i}P(i)log\frac{P(i)}{Q(i)},\]</div>
<p>and it has three properties:</p>
<ol>
<li><span class="arithmatex">\(D_{KL}(P||Q) \neq D_{KL}(Q||P)\)</span> (asymmetry)</li>
<li>it is additive for independent distributions</li>
<li><span class="arithmatex">\(D_{KL} \geq 0\)</span> with <span class="arithmatex">\(D_{KL} = 0\)</span> iff <span class="arithmatex">\(P=Q\)</span></li>
</ol>
<h2 id="working-with-documents">Working with documents<a class="headerlink" href="#working-with-documents" title="Permanent link">&para;</a></h2>
<p>We regard a document <span class="arithmatex">\(d\)</span> as discrete distribution of <span class="arithmatex">\(|d|\)</span> random variables, where <span class="arithmatex">\(|d|\)</span> is the number of words in the document. Now, let <span class="arithmatex">\(d_{1}\)</span> and <span class="arithmatex">\(d_{2}\)</span> be two documents for which we want to calculate their KL-divergence. We run into two problems:</p>
<ol>
<li>we need to compute the KL-divergence twice due to asymmetry: <span class="arithmatex">\(D_{KL}(d_{1}||d_{2})\)</span> and <span class="arithmatex">\(D_{KL}(d_{2}||d_{1})\)</span>.</li>
<li>also, due to the 2<sup>nd</sup> constraint for defining KL-divergence, our calculations should only consider words occurring in both <span class="arithmatex">\(d_{1}\)</span> and <span class="arithmatex">\(d_{2}\)</span>.</li>
</ol>
<h2 id="symmetric-kl-divergence">Symmetric KL-divergence<a class="headerlink" href="#symmetric-kl-divergence" title="Permanent link">&para;</a></h2>
<p>We start from the 2<sup>nd</sup> property of KL-divergence:</p>
<div class="arithmatex">\[\begin{array}{rcl} D_{KL}(P||Q) + D_{KL}(Q||P) &amp; = &amp; \sum_{i}P(i)log\frac{P(i)}{Q(i)} + \sum_{i}Q(i)log\frac{Q(i)}{P(i)} \\&amp; = &amp; \sum_{i}P(i)log\frac{P(i)}{Q(i)}+Q(i)log\frac{Q(i)}{P(i)}\\ &amp; = &amp; \sum_{i}P(i)log\frac{P(i)}{Q(i)}-Q(i)log\frac{P(i)}{Q(i)}\\ &amp; = &amp; \sum_{i}(P(i)-Q(i))log\frac{P(i)}{Q(i)}\end{array}\]</div>
<p>Ok! It looks good [<a href="#1">1</a>]! Now we need to compute KL-divergence only once for every pair of documents.</p>
<h2 id="over-which-random-variables">Over which random variables?<a class="headerlink" href="#over-which-random-variables" title="Permanent link">&para;</a></h2>
<p>Now let’s turn into how to handle documents with no or little overlapping vocabularies. To illustrate the problem, consider the following documents:</p>
<p><code>d1</code>:</p>
<blockquote>
<p>This is a document</p>
</blockquote>
<p><code>d2</code>:</p>
<blockquote>
<p>This is a sentence</p>
</blockquote>
<p>For each document we remove stopwords (‘this’, ‘is’, ‘a’) so they become:</p>
<p><code>d1</code>:</p>
<blockquote>
<p>document</p>
</blockquote>
<p><code>d2</code>:</p>
<blockquote>
<p>sentence</p>
</blockquote>
<p>According to constraint 2, we need to operate on the intersection of the documents’ vocabularies: <span class="arithmatex">\(d_{1}\cap d_{2}=\emptyset\)</span>. We end up with the empty set and therefore we cannot compute directly the KL-divergence. In this case we can assign it a large number like <span class="arithmatex">\(1e33\)</span>.</p>
<p>Let’s see what happens when we have larger documents.</p>
<p><code>d1</code>:</p>
<blockquote>
<p>Many research publications want you to use BibTeX which better organizes the whole process. Suppose for concreteness your source file is x.tex. Basically, you create a file x.bib containing the bibliography, and run bibtex on that file.</p>
</blockquote>
<p><code>d2</code>:</p>
<blockquote>
<p>In this case you must supply both a \left and a \right because the delimiter height are made to match whatever is contained between the two commands. But, the \left doesn&rsquo;t have to be an actual &lsquo;left delimiter&rsquo;, that is you can use &lsquo;\left)&rsquo; if there were some reason to do it.</p>
</blockquote>
<p>After stopword removal, lowercasing and discarding words less than <span class="arithmatex">\(2\)</span> characters, the documents become:</p>
<p><code>d1</code>:</p>
<blockquote>
<p>many research publications want you use bibtex better organizes whole process suppose concreteness your source file tex basically you create file bib containing bibliography run bibtex file</p>
</blockquote>
<p><code>d2</code>:</p>
<blockquote>
<p>case you must supply both left right because delimiter height made match whatever contained between two commands left doesn have actual left delimiter you use left some reason</p>
</blockquote>
<p>The vocabulary intersection of the documents consists of two terms: “use” and “you”. In <span class="arithmatex">\(d_{1}\)</span> “use” occurs 1 time and “you” occurs 2 times. Surprisingly, in <span class="arithmatex">\(d_{2}\)</span> “use” also occurs <span class="arithmatex">\(1\)</span> time and “you” occurs <span class="arithmatex">\(2\)</span> times too. The distributions <span class="arithmatex">\(D_{1}\)</span> and <span class="arithmatex">\(D_{2}\)</span> are equal, and therefore <span class="arithmatex">\(D_{KLsym}(D_{1}||D_{2}) = 0\)</span>. So these documents are deemed equal! A better stopword list could have removed “use” and “you” and in that case the documents would have an infinite KL-divergence as in the first example. However it is easy to think of similar examples where stopword lists wouldn’t have been of much help.</p>
<p>So, how can we overcome this problem?</p>
<h2 id="simple-back-off">Simple back-off<a class="headerlink" href="#simple-back-off" title="Permanent link">&para;</a></h2>
<p>Since operating on the vocabulary intersection is not an option, we need to find a trick that allows us to consider the entire vocabulary of the documents. Smoothing comes to mind. Dirichlet and Laplacian smoothing are amongst the most popular smoothing techniques but after smoothing the probability distribution doesn’t sum up to <span class="arithmatex">\(1\)</span> and violates the first constraint for defining KL-divergence.</p>
<p>Brigette Biggi [<a href="#2">2</a>] suggested a back off smoothing method which keeps the probability distributions sums to <span class="arithmatex">\(1\)</span> and also allows operating on the entire vocabulary. According to their proposed back-off method, the smoothed probability <span class="arithmatex">\(P'(t, d)\)</span> of a term <span class="arithmatex">\(t\)</span> in a document <span class="arithmatex">\(d\)</span> is:</p>
<div class="arithmatex">\[P'(t_{i},d) = \left\{ \begin{array}{ll} \gamma P(t_{i}|d) &amp; \quad \text{if it occurs in d}\\ \epsilon &amp; \quad \text{otherwise}\\ \end{array} \right.\]</div>
<p>where</p>
<div class="arithmatex">\[P(t_{i}|d) = \frac{tf(t_{i}, d)}{\sum_{x\in d}tf(t_{x},d)}\]</div>
<p>the interesting part is on how <span class="arithmatex">\(\gamma\)</span> and <span class="arithmatex">\(\epsilon\)</span> are calculated. In order to keep the term probabilities for <span class="arithmatex">\(d_{1}\)</span> and <span class="arithmatex">\(d_{2}\)</span> summing up to <span class="arithmatex">\(1\)</span>, the following constraint should be met:</p>
<div class="arithmatex">\[\sum_{i \in d}\gamma P(t_{i}|d) + \sum_{i \in d_{1}, i \notin d_{2}}\epsilon = 1\]</div>
<p>where <span class="arithmatex">\(\gamma\)</span> is a normalization coefficient:</p>
<div class="arithmatex">\[\gamma = 1 - \sum_{i \in d_{1}, i \notin d_{2}}\epsilon\]</div>
<p>and <span class="arithmatex">\(\epsilon\)</span> is a positive number smaller than the minimum term probability occurring in either <span class="arithmatex">\(d_{1}\)</span> or <span class="arithmatex">\(d_{2}\)</span>.</p>
<h2 id="the-code">The code<a class="headerlink" href="#the-code" title="Permanent link">&para;</a></h2>
<p>To illustrate the above, I wrote a small Python script:</p>
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">re</span><span style="color: #666666">,</span> <span style="color: #0000FF; font-weight: bold">math</span><span style="color: #666666">,</span> <span style="color: #0000FF; font-weight: bold">collections</span>

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">tokenize</span>(_str):
    stopwords <span style="color: #666666">=</span> [<span style="color: #BA2121">&#39;and&#39;</span>, <span style="color: #BA2121">&#39;for&#39;</span>, <span style="color: #BA2121">&#39;if&#39;</span>, <span style="color: #BA2121">&#39;the&#39;</span>, <span style="color: #BA2121">&#39;then&#39;</span>, <span style="color: #BA2121">&#39;be&#39;</span>, <span style="color: #BA2121">&#39;is&#39;</span>, <span style="color: #BA2121">&#39;are&#39;</span>, <span style="color: #BA2121">&#39;will&#39;</span>, <span style="color: #BA2121">&#39;in&#39;</span>, <span style="color: #BA2121">&#39;it&#39;</span>, <span style="color: #BA2121">&#39;to&#39;</span>, <span style="color: #BA2121">&#39;that&#39;</span>]
    tokens <span style="color: #666666">=</span> collections<span style="color: #666666">.</span>defaultdict(<span style="color: #008000; font-weight: bold">lambda</span>: <span style="color: #666666">0.</span>)
    <span style="color: #008000; font-weight: bold">for</span> m <span style="color: #AA22FF; font-weight: bold">in</span> re<span style="color: #666666">.</span>finditer(<span style="color: #BA2121">r&quot;(\w+)&quot;</span>, _str, re<span style="color: #666666">.</span>UNICODE):
        m <span style="color: #666666">=</span> m<span style="color: #666666">.</span>group(<span style="color: #666666">1</span>)<span style="color: #666666">.</span>lower()
        <span style="color: #008000; font-weight: bold">if</span> <span style="color: #008000">len</span>(m) <span style="color: #666666">&lt;</span> <span style="color: #666666">2</span>: <span style="color: #008000; font-weight: bold">continue</span>
        <span style="color: #008000; font-weight: bold">if</span> m <span style="color: #AA22FF; font-weight: bold">in</span> stopwords: <span style="color: #008000; font-weight: bold">continue</span>
        tokens[m] <span style="color: #666666">+=</span> <span style="color: #666666">1</span>

    <span style="color: #008000; font-weight: bold">return</span> tokens
<span style="color: #3D7B7B; font-style: italic">#end of tokenize</span>

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">kldiv</span>(_s, _t):
    <span style="color: #008000; font-weight: bold">if</span> (<span style="color: #008000">len</span>(_s) <span style="color: #666666">==</span> <span style="color: #666666">0</span>):
        <span style="color: #008000; font-weight: bold">return</span> <span style="color: #666666">1e33</span>

    <span style="color: #008000; font-weight: bold">if</span> (<span style="color: #008000">len</span>(_t) <span style="color: #666666">==</span> <span style="color: #666666">0</span>):
        <span style="color: #008000; font-weight: bold">return</span> <span style="color: #666666">1e33</span>

    ssum <span style="color: #666666">=</span> <span style="color: #666666">0.</span> <span style="color: #666666">+</span> <span style="color: #008000">sum</span>(_s<span style="color: #666666">.</span>values())
    slen <span style="color: #666666">=</span> <span style="color: #008000">len</span>(_s)

    tsum <span style="color: #666666">=</span> <span style="color: #666666">0.</span> <span style="color: #666666">+</span> <span style="color: #008000">sum</span>(_t<span style="color: #666666">.</span>values())
    tlen <span style="color: #666666">=</span> <span style="color: #008000">len</span>(_t)

    vocabdiff <span style="color: #666666">=</span> <span style="color: #008000">set</span>(_s<span style="color: #666666">.</span>keys())<span style="color: #666666">.</span>difference(<span style="color: #008000">set</span>(_t<span style="color: #666666">.</span>keys()))
    lenvocabdiff <span style="color: #666666">=</span> <span style="color: #008000">len</span>(vocabdiff)

<span style="color: #bbbbbb">    </span><span style="color: #BA2121; font-style: italic">&quot;&quot;&quot; epsilon &quot;&quot;&quot;</span>
    epsilon <span style="color: #666666">=</span> <span style="color: #008000">min</span>(<span style="color: #008000">min</span>(_s<span style="color: #666666">.</span>values())<span style="color: #666666">/</span>ssum, <span style="color: #008000">min</span>(_t<span style="color: #666666">.</span>values())<span style="color: #666666">/</span>tsum) <span style="color: #666666">*</span> <span style="color: #666666">0.001</span>

<span style="color: #bbbbbb">    </span><span style="color: #BA2121; font-style: italic">&quot;&quot;&quot; gamma &quot;&quot;&quot;</span>
    gamma <span style="color: #666666">=</span> <span style="color: #666666">1</span> <span style="color: #666666">-</span> lenvocabdiff <span style="color: #666666">*</span> epsilon

    <span style="color: #3D7B7B; font-style: italic"># print &quot;_s: %s&quot; % _s</span>
    <span style="color: #3D7B7B; font-style: italic"># print &quot;_t: %s&quot; % _t</span>

<span style="color: #bbbbbb">    </span><span style="color: #BA2121; font-style: italic">&quot;&quot;&quot; Check if distribution probabilities sum to 1&quot;&quot;&quot;</span>
    sc <span style="color: #666666">=</span> <span style="color: #008000">sum</span>([v<span style="color: #666666">/</span>ssum <span style="color: #008000; font-weight: bold">for</span> v <span style="color: #AA22FF; font-weight: bold">in</span> _s<span style="color: #666666">.</span>itervalues()])
    st <span style="color: #666666">=</span> <span style="color: #008000">sum</span>([v<span style="color: #666666">/</span>tsum <span style="color: #008000; font-weight: bold">for</span> v <span style="color: #AA22FF; font-weight: bold">in</span> _t<span style="color: #666666">.</span>itervalues()])

    <span style="color: #008000; font-weight: bold">if</span> sc <span style="color: #666666">&lt;</span> <span style="color: #666666">9e-6</span>:
        <span style="color: #008000">print</span> <span style="color: #BA2121">&quot;Sum P: </span><span style="color: #A45A77; font-weight: bold">%e</span><span style="color: #BA2121">, Sum Q: </span><span style="color: #A45A77; font-weight: bold">%e</span><span style="color: #BA2121">&quot;</span> <span style="color: #666666">%</span> (sc, st)
        <span style="color: #008000">print</span> <span style="color: #BA2121">&quot;*** ERROR: sc does not sum up to 1. Bailing out ..&quot;</span>
        sys<span style="color: #666666">.</span>exit(<span style="color: #666666">2</span>)
    <span style="color: #008000; font-weight: bold">if</span> st <span style="color: #666666">&lt;</span> <span style="color: #666666">9e-6</span>:
        <span style="color: #008000">print</span> <span style="color: #BA2121">&quot;Sum P: </span><span style="color: #A45A77; font-weight: bold">%e</span><span style="color: #BA2121">, Sum Q: </span><span style="color: #A45A77; font-weight: bold">%e</span><span style="color: #BA2121">&quot;</span> <span style="color: #666666">%</span> (sc, st)
        <span style="color: #008000">print</span> <span style="color: #BA2121">&quot;*** ERROR: st does not sum up to 1. Bailing out ..&quot;</span>
        sys<span style="color: #666666">.</span>exit(<span style="color: #666666">2</span>)

    div <span style="color: #666666">=</span> <span style="color: #666666">0.</span>
    <span style="color: #008000; font-weight: bold">for</span> t, v <span style="color: #AA22FF; font-weight: bold">in</span> _s<span style="color: #666666">.</span>iteritems():
        pts <span style="color: #666666">=</span> v <span style="color: #666666">/</span> ssum

        ptt <span style="color: #666666">=</span> epsilon
        <span style="color: #008000; font-weight: bold">if</span> t <span style="color: #AA22FF; font-weight: bold">in</span> _t:
            ptt <span style="color: #666666">=</span> gamma <span style="color: #666666">*</span> (_t[t] <span style="color: #666666">/</span> tsum)

        ckl <span style="color: #666666">=</span> (pts <span style="color: #666666">-</span> ptt) <span style="color: #666666">*</span> math<span style="color: #666666">.</span>log(pts <span style="color: #666666">/</span> ptt)

        div <span style="color: #666666">+=</span>  ckl

    <span style="color: #008000; font-weight: bold">return</span> div
<span style="color: #3D7B7B; font-style: italic">#end of kldiv</span>

d1 <span style="color: #666666">=</span> <span style="color: #BA2121">&quot;&quot;&quot;Many research publications want you to use BibTeX, which better</span>
<span style="color: #BA2121">organizes the whole process. Suppose for concreteness your source</span>
<span style="color: #BA2121">file is x.tex. Basically, you create a file x.bib containing the</span>
<span style="color: #BA2121">bibliography, and run bibtex on that file.&quot;&quot;&quot;</span>

d2 <span style="color: #666666">=</span> <span style="color: #BA2121">&quot;&quot;&quot;In this case you must supply both a \left and a </span><span style="color: #AA5D1F; font-weight: bold">\r</span><span style="color: #BA2121">ight because the</span>
<span style="color: #BA2121">delimiter height are made to match whatever is contained between the</span>
<span style="color: #BA2121">two commands. But, the \left doesn&#39;t have to be an actual &#39;left</span>
<span style="color: #BA2121">delimiter&#39;, that is you can use &#39;\left)&#39; if there were some reason</span>
<span style="color: #BA2121">to do it.&quot;&quot;&quot;</span>

<span style="color: #008000">print</span> <span style="color: #BA2121">&quot;KL-divergence between d1 and d2:&quot;</span>, kldiv(tokenize(d1), tokenize(d2))
<span style="color: #008000">print</span> <span style="color: #BA2121">&quot;KL-divergence between d2 and d1:&quot;</span>, kldiv(tokenize(d2), tokenize(d1))
</code></pre></div>
<p>The output looks like this:</p>
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code>KL-divergence between d1 and d2: 6.52185430964
KL-divergence between d2 and d1: 6.51142363095
</code></pre></div>
<p>Now, KL-divergence is greater than zero, so the documents are not thought to be the same as before! Good job. Looking at KL symmetry, although the divergence of the two pairs is not identical, it is sufficiently close [<a href="#1">1</a>].</p>
<h4 id="acknowledgments">Acknowledgments<a class="headerlink" href="#acknowledgments" title="Permanent link">&para;</a></h4>
<p>The above is a compilation of knowledge found in Wikipedia and from the paper “Reducing the Plagiarism Detection Search Space on the basis of Kullback-Leibler Distance” by Alberto Barron-Cedeno, Paolo Rosso, and Jose-Miguel Benedi [<a href="#1">1</a>]. Examples and code are mine and you are free to use them.</p>
<h3 id="references">References<a class="headerlink" href="#references" title="Permanent link">&para;</a></h3>
<p><span id="1"/>[1] Pinto, D., Benedí, JM., Rosso, P. (2007). <em>Clustering Narrow-Domain Short Texts by Using the Kullback-Leibler Distance</em>. In Computational Linguistics and Intelligent Text Processing (CICLing 2007). Lecture Notes in Computer Science, vol 4394. Springer, Berlin, Heidelberg. <a href="https://link.springer.com/chapter/10.1007/978-3-540-70939-8_54">Link</a></p>
<p><span id="2"/>[2] Bigi, B. (2003). <em>Using Kullback-Leibler Distance for Text Categorization</em>. In Advances in Information Retrieval (ECIR 2003). Lecture Notes in Computer Science, vol 2633. Springer, Berlin, Heidelberg. <a href="https://doi.org/10.1007/3-540-36618-0_22">Link</a></p>
</article>

<hr/>


<div class="related-links">
    <h3>Related Posts</h3>
    <ol>
        
        <li>
            
            <blockquote>
                <p>&nbsp;
To make up for this discrepancy, we consider distributions that emerge from sampling without replacement: the central and non-central hypergeometric distributions. We present two retrieval models that build on top of these distributions: a log odds model and a bayesian model where document parameters are estimated using the Dirichlet compound multinomial distribution.
&nbsp;
We analyse the behavior of our new models using a corpus of news articles and blog posts and find that for the task of republished article finding, where we deal with queries whose length approaches the length of the documents to be retrieved, models based on distributions associated with sampling without replacement outperform traditional models based on multinomial distributions.</p>
            </blockquote>
            
            <div class="snippet-meta">
                <span class="score">4.58% similar</span>
                <span>— <a href="/posts/20110408-sigir2011-hypergeometric-language-models/">Paper at SIGIR 2011 — Hypergeometric Language Models for Republished Article Finding</a></span>
            </div>
        </li>
        
        <li>
            
            <blockquote>
                <h3 id="references">References</h3>
<p><span id="1">[1] Manos Tsagkias, Martha Larson, Wouter Weerkamp, and Maarten de Rijke. 2008. PodCred: a framework for analyzing podcast preference. In Proceedings of the 2<sup>nd</sup> ACM Workshop on Information Credibility On the Web (WICOW &lsquo;08). Association for Computing Machinery, New York, NY, USA, 67–74. <a href="https://doi.org/10.1145/1458527.1458545">ACM link</a> <a href="../pdf/wicow2008-podcred.pdf">PDF</a></span></p>
            </blockquote>
            
            <div class="snippet-meta">
                <span class="score">3.54% similar</span>
                <span>— <a href="/posts/20081120-wicow2008-podcred/">Paper at WICOW 2008 — PodCred: A Framework for Analyzing Podcast Preference</a></span>
            </div>
        </li>
        
        <li>
            
            <blockquote>
                <h3 id="abstract">Abstract</h3>
<p>Podcasts display an unevenness characteristic of domains dominated by user generated content, resulting in potentially radical variation of the user preference they enjoy. We report on work that uses easily extractable surface features of podcasts in order to achieve solid performance on two podcast preference prediction tasks: classification of preferred vs. non-preferred podcasts and ranking podcasts by level of preference. We identify features with good discriminative potential by carrying out manual data analysis, resulting in a refinement of the indicators of an existent podcast preference framework. Our preference prediction is useful for topic-independent ranking of podcasts, and can be used to support download suggestion or collection browsing.</p>
<h3 id="references">References</h3>
            </blockquote>
            
            <div class="snippet-meta">
                <span class="score">2.51% similar</span>
                <span>— <a href="/posts/20090304-ecir2009-podcred-features/">Paper at ECIR 2009 — Exploiting Surface Features for the Prediction of Podcast Preference</a></span>
            </div>
        </li>
        
        <li>
            
            <blockquote>
                <h3 id="abstract">Abstract</h3>
<p>We propose a retrieval model for searching microblog posts for a given topic of interest. We develop a language modeling approach tailored to microblogging characteristics, where redundancy-based IR methods cannot be used in a straightforward manner. We enhance this model with two groups of quality indicators: textual and microblog specific. Additionally, we propose a dynamic query expansion model for microblog post retrieval. Experimental results on Twitter data reveal the usefulness of boolean search, and demonstrate the utility of quality indicators and query expansion in microblog search.</p>
<h3 id="references">References</h3>
            </blockquote>
            
            <div class="snippet-meta">
                <span class="score">2.08% similar</span>
                <span>— <a href="/posts/20110119-ecir2011-query-expansion/">Paper at ECIR 2011 — Incorporating Query Expansion and Quality Indicators in Searching Microblog Posts</a></span>
            </div>
        </li>
        
        <li>
            
            <blockquote>
                <h3 id="abstract">Abstract</h3>
<p>User generated spoken audio remains a challenge for Automatic Speech Recognition (ASR) technology and content-based audio surrogates derived from ASR-transcripts must be error robust. An investigation of the use of term clouds as surrogates for podcasts demonstrates that ASR term clouds closely approximate term clouds derived from human-generated transcripts across a range of cloud sizes. A user study confirms the conclusion that ASR-clouds are viable surrogates for depicting the content of podcasts.</p>
<h3 id="references">References</h3>
<p><span id="1">[1] Manos Tsagkias, Martha Larson, and Maarten de Rijke. 2008. <em>Term clouds as surrogates for user generated speech</em>. In Proceedings of the 31<sup>st</sup> annual international ACM SIGIR conference on Research and development in information retrieval (SIGIR &lsquo;08). Association for Computing Machinery, New York, NY, USA, 773–774. <a href="https://doi.org/10.1145/1390334.1390497">ACM Link</a> <a href="../pdf/sigir2008-termclouds.pdf">PDF</a></span></p>
            </blockquote>
            
            <div class="snippet-meta">
                <span class="score">1.89% similar</span>
                <span>— <a href="/posts/20080620-sigir2008-speech-termclouds/">Paper at SIGIR 2008 — Term Clouds as Surrogates for User Generated Speech</a></span>
            </div>
        </li>
        
        <li>
            
            <blockquote>
                <p>We describe the participation of the University of Amsterdam’s ILPS group in the web, blog, web, entity, and relevance feedback track at TREC 2009. Our main preliminary conclusions are as follows. For the Blog track we find that for top stories identification a blogs to news approach outperforms a simple news to blogs approach. This is interesting, as this approach starts with no input except for a date, whereas the news to blogs approach also has news headlines as input. For the web track, we find that spam is an important issue in the ad hoc task and that Wikipedia- based heuristic optimization approaches help to boost the retrieval performance, which is assumed to potentially reduce the spam in top ranked documents. As for the diversity task, we explored different methods. Initial results show that clustering and a topic model-based approach have similar performance, which are relatively better than a query log based approach. Our performance in the Entity track was downright</p>
            </blockquote>
            
            <div class="snippet-meta">
                <span class="score">1.82% similar</span>
                <span>— <a href="/posts/20091026-trec2009/">Paper at TREC 2009 — The University of Amsterdam at TREC 2009</a></span>
            </div>
        </li>
        
        <li>
            
            <blockquote>
                <p>The Dutch-Belgian Information Retrieval Workshop was held this year in Delft. A nice lot of people from all around the Netherlands, Belgium, and a handful from abroad came together to share their research highlights from this year. It was a nice programme, filled with interesting short talks (about 15mins) that were great to give us a summary of what everybody is working on.</p>
<p>My personal highlight was Chato&rsquo;s keynote speech on algorithmic bias. Although a lot is discussed on the topic, there is yet no to little work that presents some sort of foundation that describes the issue, frames it and offers some theoretical framework from which we can start thinking about solutions. Chato&rsquo;s work laid the ground works on this area. He started with explaining what algorithmic bias is and stressed how coupled it is with data collection. I&rsquo;m looking forward to seeing more on his work on the subject.</p>
            </blockquote>
            
            <div class="snippet-meta">
                <span class="score">1.42% similar</span>
                <span>— <a href="/posts/20161210-dir/">Attending DIR 2016</a></span>
            </div>
        </li>
        
    </ol>
</div>

    </main>
    
    <footer>
        <p>
            Powered by MkDocs; design follows <a href="https://github.com/goessner">Stefan Gössner</a>'s <a href="https://github.com/goessner/mdmath">md-math</a>.
        </p>
    </footer>
      <script src="https://d3js.org/d3.v7.min.js"></script>
      <script src="https://cdn.jsdelivr.net/npm/dexie/dist/dexie.min.js"></script>
      <script src="../../js/d3-umap.js"></script>
      <script src="../../search/main.js"></script>
    <script type="module">
        import { search } from '/js/onnx.js';
    </script>
</body>
</html>